{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import utm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 38 # Seed for train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data paths\n",
    "train_data = 'Example_data\\Output\\Train_test_data\\Training_Tag_GPS_locations.xlsx'\n",
    "test_data = 'Example_data\\Output\\Train_test_data\\Testing_Tag_GPS_locations.xlsx'\n",
    "radio_tower_xy_path = 'H:\\My Drive\\Colab Notebooks\\RadioTelemetry\\Tower_data\\RTEastNorth.xlsx'\n",
    "\n",
    "# Variable parameters\n",
    "frequencies = ['1min'] # Add more for final run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to convert geographic to projected coordinates\n",
    "def from_latlon(lat, lon):\n",
    "    easting, northing, zone_num, zone_letter = utm.from_latlon(lat, lon)\n",
    "    return easting, northing, zone_num, zone_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sim_data(sim_data, data_type, freq, tower_locs):\n",
    "    # Get data\n",
    "    sim_dat_filt = sim_data[sim_data['Data_type'].isin(data_type)]\n",
    "    \n",
    "    # make column with the datetime to nearest 'freq' value (e.g. 5min)\n",
    "    sim_dat_filt = sim_dat_filt.assign(DateTime = sim_dat_filt['DateAndTime'].dt.floor(freq=freq))\n",
    "  \n",
    "    # group by datetime, tag, tower and antenna, compute mean power and std power, pivot to antennas\n",
    "    sim_dat_filt = (\n",
    "            sim_dat_filt.groupby(['DateTime', 'TowerID', 'TagID', 'Antenna', 'POINT_X', 'POINT_Y'])['Power']\n",
    "            .agg(['mean', 'count', np.std])\n",
    "            .reset_index()\n",
    "            .pivot_table(index=['DateTime', 'TowerID', 'TagID', 'POINT_X', 'POINT_Y'], columns='Antenna', values=['mean', 'count', 'std'])\n",
    "            .reset_index()\n",
    "        )\n",
    "    \n",
    "    # Rename columns\n",
    "    sim_dat_filt.columns = [f\"{col[0]}{col[1]}\" if col[1] != \"\" else col[0] for col in sim_dat_filt.columns.values]\n",
    "    sim_dat_filt = sim_dat_filt.rename(columns={ 'mean1': 'ant1_mean', 'mean2': 'ant2_mean', 'mean3': 'ant3_mean', 'mean4': 'ant4_mean',\n",
    "                                                  'count1': 'ant1_count', 'count2': 'ant2_count', 'count3': 'ant3_count', 'count4': 'ant4_count',\n",
    "                                                  'std1': 'ant1_std', 'std2': 'ant2_std', 'std3': 'ant3_std', 'std4': 'ant4_std'})\n",
    "    \n",
    "    # Calculate the mean std and total count across the antennas\n",
    "    sim_dat_filt['mean_std'] = sim_dat_filt[['ant1_std', 'ant2_std', 'ant3_std', 'ant4_std']].mean(axis=1)\n",
    "    sim_dat_filt['total_count'] = sim_dat_filt[['ant1_count', 'ant2_count', 'ant3_count', 'ant4_count']].sum(axis=1)\n",
    "\n",
    "    # Fill missing values with 0\n",
    "    sim_dat_filt = sim_dat_filt.fillna(value=0)\n",
    "     \n",
    "    # Calculate easting and northing from lat long\n",
    "    sim_dat_filt['easting'], sim_dat_filt['northing'], sim_dat_filt['zone_num'], sim_dat_filt['zone_letter'] = from_latlon(sim_dat_filt['POINT_Y'].values, sim_dat_filt['POINT_X'].values)\n",
    "\n",
    "    # Create a dictionary of the coordinates of the towers\n",
    "    offset_dict = tower_locs.set_index('TowerID').to_dict()\n",
    "    point_x = offset_dict['POINT_X']\n",
    "    point_y = offset_dict['POINT_Y']\n",
    "\n",
    "    # Standardise the coordinates so that the tower location == 0 on both the x and y axes.\n",
    "    sim_dat_filt['xOffset'] = sim_dat_filt['easting'] - sim_dat_filt['TowerID'].map(point_x).fillna(0)\n",
    "    sim_dat_filt['yOffset'] = sim_dat_filt['northing'] - sim_dat_filt['TowerID'].map(point_y).fillna(0)\n",
    "    \n",
    "    return sim_dat_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data\n",
    "train_data = pd.read_excel(train_data)\n",
    "train_data['DateAndTime'] = pd.to_datetime(train_data['DateAndTime'])\n",
    "\n",
    "# Get testing data\n",
    "test_data = pd.read_excel(test_data)\n",
    "test_data['DateAndTime'] = pd.to_datetime(test_data['DateAndTime'])\n",
    "\n",
    "# Get tower locations\n",
    "tower_locs = pd.read_excel(radio_tower_xy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pre-processing parameters to be used\n",
    "\n",
    "# Fixed parameters\n",
    "data_type = ['Simulated BTFS', 'BTFS'] # Simulation or Live BTF, or could do both\n",
    "dimensions = ['xOffset', 'yOffset']\n",
    "predictors = ['ant1_mean', 'ant2_mean', 'ant3_mean', 'ant4_mean', 'ant1_count', 'ant2_count', 'ant3_count', 'ant4_count', 'ant1_std', 'ant2_std', 'ant3_std', 'ant4_std', 'mean_std', 'total_count']\n",
    "responses = ['xOffset', 'yOffset']\n",
    "scoring = 'neg_mean_absolute_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                             \n",
      "Generation 1 - Current best internal CV score: -234.45353824749026\n",
      "                                                                              \n",
      "Generation 2 - Current best internal CV score: -234.45353824749026\n",
      "                                                                              \n",
      "Generation 3 - Current best internal CV score: -234.45353824749026\n",
      "                                                                              \n",
      "Generation 4 - Current best internal CV score: -233.92694733558378\n",
      "                                                                              \n",
      "Generation 5 - Current best internal CV score: -233.92694733558378\n",
      "                                                                              \n",
      "Best pipeline: SGDRegressor(LassoLarsCV(ExtraTreesRegressor(input_matrix, bootstrap=False, max_features=0.55, min_samples_leaf=17, min_samples_split=15, n_estimators=100), normalize=True), alpha=0.0, eta0=0.01, fit_intercept=True, l1_ratio=0.0, learning_rate=invscaling, loss=huber, penalty=elasticnet, power_t=0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s5236256\\Documents\\GitHub\\ml4rt\\.venv\\Lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but ExtraTreesRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\s5236256\\Documents\\GitHub\\ml4rt\\.venv\\Lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but ExtraTreesRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-226.7230535201966\n",
      "0.06154662221367646\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.ensemble import ExtraTreesRegressor\n",
      "from sklearn.linear_model import LassoLarsCV, SGDRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.pipeline import make_pipeline, make_union\n",
      "from tpot.builtins import StackingEstimator\n",
      "from tpot.export_utils import set_param_recursive\n",
      "\n",
      "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
      "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
      "features = tpot_data.drop('target', axis=1)\n",
      "training_features, testing_features, training_target, testing_target = \\\n",
      "            train_test_split(features, tpot_data['target'], random_state=38)\n",
      "\n",
      "# Average CV score on the training set was: -233.92694733558378\n",
      "exported_pipeline = make_pipeline(\n",
      "    StackingEstimator(estimator=ExtraTreesRegressor(bootstrap=False, max_features=0.55, min_samples_leaf=17, min_samples_split=15, n_estimators=100)),\n",
      "    StackingEstimator(estimator=LassoLarsCV(normalize=True)),\n",
      "    SGDRegressor(alpha=0.0, eta0=0.01, fit_intercept=True, l1_ratio=0.0, learning_rate=\"invscaling\", loss=\"huber\", penalty=\"elasticnet\", power_t=0.5)\n",
      ")\n",
      "# Fix random state for all the steps in exported pipeline\n",
      "set_param_recursive(exported_pipeline.steps, 'random_state', 38)\n",
      "\n",
      "exported_pipeline.fit(training_features, training_target)\n",
      "results = exported_pipeline.predict(testing_features)\n",
      "\n",
      "                                                                              \n",
      "Generation 1 - Current best internal CV score: -174.62253687319028\n",
      "                                                                              \n",
      "Generation 2 - Current best internal CV score: -174.62253687319028\n",
      "                                                                              \n",
      "Generation 3 - Current best internal CV score: -171.9958755471745\n",
      "                                                                              \n",
      "Generation 4 - Current best internal CV score: -171.9958755471745\n",
      "                                                                              \n",
      "Generation 5 - Current best internal CV score: -169.35637944939668\n",
      "                                                                              \n",
      "Best pipeline: KNeighborsRegressor(RandomForestRegressor(input_matrix, bootstrap=True, max_features=0.55, min_samples_leaf=17, min_samples_split=19, n_estimators=100), n_neighbors=26, p=1, weights=uniform)\n",
      "-180.83909325210826\n",
      "0.12060165669497802\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.neighbors import KNeighborsRegressor\n",
      "from sklearn.pipeline import make_pipeline, make_union\n",
      "from tpot.builtins import StackingEstimator\n",
      "from tpot.export_utils import set_param_recursive\n",
      "\n",
      "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
      "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
      "features = tpot_data.drop('target', axis=1)\n",
      "training_features, testing_features, training_target, testing_target = \\\n",
      "            train_test_split(features, tpot_data['target'], random_state=38)\n",
      "\n",
      "# Average CV score on the training set was: -169.35637944939668\n",
      "exported_pipeline = make_pipeline(\n",
      "    StackingEstimator(estimator=RandomForestRegressor(bootstrap=True, max_features=0.55, min_samples_leaf=17, min_samples_split=19, n_estimators=100)),\n",
      "    KNeighborsRegressor(n_neighbors=26, p=1, weights=\"uniform\")\n",
      ")\n",
      "# Fix random state for all the steps in exported pipeline\n",
      "set_param_recursive(exported_pipeline.steps, 'random_state', 38)\n",
      "\n",
      "exported_pipeline.fit(training_features, training_target)\n",
      "results = exported_pipeline.predict(testing_features)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s5236256\\Documents\\GitHub\\ml4rt\\.venv\\Lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\s5236256\\Documents\\GitHub\\ml4rt\\.venv\\Lib\\site-packages\\sklearn\\base.py:420: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initial pass over the data using lazy predict to identify possible options\n",
    "freq = '5min'\n",
    "\n",
    "for dimension in dimensions:\n",
    "    sim_data_preproc = preprocess_sim_data(train_data, data_type, freq, tower_locs)\n",
    "    X_train = sim_data_preproc[predictors]\n",
    "    y_train = sim_data_preproc[dimension] # Will need to adjust this to iterate over x and y xOffset\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=seed)\n",
    "\n",
    "    tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2, scoring=scoring, random_state=seed)\n",
    "    tpot.fit(X_train, y_train)\n",
    "    print(tpot.score(X_test, y_test))\n",
    "    preds = tpot.predict(X_test)\n",
    "    print(r2_score(y_test, preds))\n",
    "    tpot.export(f'{dimension}_tpot_pipeline.py')\n",
    "    print(tpot.export())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c9c44150708f88dcc61b6a40c517040b165081f9891458623fe805b4ae9321d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
