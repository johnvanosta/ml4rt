{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import utm\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "from utils.from_latlon import from_latlon\n",
    "from utils.preprocessing import preprocess\n",
    "from utils.postprocessing import postprocess_data, location_averaging, calculate_error\n",
    "\n",
    "seed = 38 # Seed for train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data paths\n",
    "train_data = 'SBTF_data/input_data/train_test_data/train_data_combined.xlsx'\n",
    "test_data = 'SBTF_data/input_data/train_test_data/test_data.xlsx'\n",
    "radio_tower_xy_path = 'SBTF_data/input_data/radio_tower_locations/RTEastNorth_1group.xlsx'\n",
    "\n",
    "# Variable parameters\n",
    "freq = '3min' # Frequency of data\n",
    "\n",
    "# Fixed parameters\n",
    "routine = 'training'\n",
    "dimensions = ['xOffset', 'yOffset']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sim_data(sim_data, freq, tower_locs, routine):\n",
    "    \n",
    "    sim_dat_filt, predictors = preprocess(sim_data, freq, routine)\n",
    "     \n",
    "    # Calculate easting and northing from lat long\n",
    "    sim_dat_filt['easting'], sim_dat_filt['northing'], sim_dat_filt['zone_num'], sim_dat_filt['zone_letter'] = from_latlon(sim_dat_filt['POINT_Y'].values, sim_dat_filt['POINT_X'].values)\n",
    "\n",
    "    # Create a dictionary of the coordinates of the towers\n",
    "    offset_dict = tower_locs.set_index('TowerID').to_dict()\n",
    "    point_x = offset_dict['POINT_X']\n",
    "    point_y = offset_dict['POINT_Y']\n",
    "\n",
    "    # Standardise the coordinates so that the tower location == 0 on both the x and y axes.\n",
    "    sim_dat_filt['xOffset'] = sim_dat_filt['easting'] - sim_dat_filt['TowerID'].map(point_x).fillna(0)\n",
    "    sim_dat_filt['yOffset'] = sim_dat_filt['northing'] - sim_dat_filt['TowerID'].map(point_y).fillna(0)\n",
    "    \n",
    "    return sim_dat_filt, predictors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data\n",
    "train_data = pd.read_excel(train_data)\n",
    "train_data['DateAndTime'] = pd.to_datetime(train_data['DateAndTime'])\n",
    "\n",
    "# Get testing data\n",
    "test_data = pd.read_excel(test_data)\n",
    "test_data['DateAndTime'] = pd.to_datetime(test_data['DateAndTime'])\n",
    "\n",
    "# Get tower locations\n",
    "tower_locs = pd.read_excel(radio_tower_xy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s5236256\\Documents\\GitHub\\ml4rt\\utils\\preprocessing.py:10: FutureWarning: The provided callable <function std at 0x00000275B3F80A60> is currently using SeriesGroupBy.std. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"std\" instead.\n",
      "  .agg(['mean', 'count', np.std])\n"
     ]
    }
   ],
   "source": [
    "train_data_preproc, predictors_train = preprocess_sim_data(train_data, freq, tower_locs, routine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_preproc['Point_ID'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\s5236256\\Documents\\GitHub\\ml4rt\\utils\\preprocessing.py:10: FutureWarning: The provided callable <function std at 0x00000275B3F80A60> is currently using SeriesGroupBy.std. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"std\" instead.\n",
      "  .agg(['mean', 'count', np.std])\n",
      "c:\\Users\\s5236256\\Documents\\GitHub\\ml4rt\\utils\\preprocessing.py:10: FutureWarning: The provided callable <function std at 0x00000275B3F80A60> is currently using SeriesGroupBy.std. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"std\" instead.\n",
      "  .agg(['mean', 'count', np.std])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... not found.\n",
      "Attempting to start a local H2O server...\n",
      "; OpenJDK 64-Bit Server VM Zulu17.42+19-CA (build 17.0.7+7-LTS, mixed mode, sharing)\n",
      "  Starting server from C:\\Users\\s5236256\\Documents\\GitHub\\ml4rt\\.venv\\Lib\\site-packages\\h2o\\backend\\bin\\h2o.jar\n",
      "  Ice root: C:\\Users\\s5236256\\AppData\\Local\\Temp\\tmpohl6ow5p\n",
      "  JVM stdout: C:\\Users\\s5236256\\AppData\\Local\\Temp\\tmpohl6ow5p\\h2o_s5236256_started_from_python.out\n",
      "  JVM stderr: C:\\Users\\s5236256\\AppData\\Local\\Temp\\tmpohl6ow5p\\h2o_s5236256_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       "#h2o-table-1.h2o-container {\n",
       "  overflow-x: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table {\n",
       "  /* width: 100%; */\n",
       "  margin-top: 1em;\n",
       "  margin-bottom: 1em;\n",
       "}\n",
       "#h2o-table-1 .h2o-table caption {\n",
       "  white-space: nowrap;\n",
       "  caption-side: top;\n",
       "  text-align: left;\n",
       "  /* margin-left: 1em; */\n",
       "  margin: 0;\n",
       "  font-size: larger;\n",
       "}\n",
       "#h2o-table-1 .h2o-table thead {\n",
       "  white-space: nowrap; \n",
       "  position: sticky;\n",
       "  top: 0;\n",
       "  box-shadow: 0 -1px inset;\n",
       "}\n",
       "#h2o-table-1 .h2o-table tbody {\n",
       "  overflow: auto;\n",
       "}\n",
       "#h2o-table-1 .h2o-table th,\n",
       "#h2o-table-1 .h2o-table td {\n",
       "  text-align: right;\n",
       "  /* border: 1px solid; */\n",
       "}\n",
       "#h2o-table-1 .h2o-table tr:nth-child(even) {\n",
       "  /* background: #F5F5F5 */\n",
       "}\n",
       "\n",
       "</style>      \n",
       "<div id=\"h2o-table-1\" class=\"h2o-container\">\n",
       "  <table class=\"h2o-table\">\n",
       "    <caption></caption>\n",
       "    <thead></thead>\n",
       "    <tbody><tr><td>H2O_cluster_uptime:</td>\n",
       "<td>05 secs</td></tr>\n",
       "<tr><td>H2O_cluster_timezone:</td>\n",
       "<td>Australia/Brisbane</td></tr>\n",
       "<tr><td>H2O_data_parsing_timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O_cluster_version:</td>\n",
       "<td>3.44.0.3</td></tr>\n",
       "<tr><td>H2O_cluster_version_age:</td>\n",
       "<td>1 month and 8 days</td></tr>\n",
       "<tr><td>H2O_cluster_name:</td>\n",
       "<td>H2O_from_python_s5236256_yvrl5q</td></tr>\n",
       "<tr><td>H2O_cluster_total_nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O_cluster_free_memory:</td>\n",
       "<td>3.922 Gb</td></tr>\n",
       "<tr><td>H2O_cluster_total_cores:</td>\n",
       "<td>0</td></tr>\n",
       "<tr><td>H2O_cluster_allowed_cores:</td>\n",
       "<td>0</td></tr>\n",
       "<tr><td>H2O_cluster_status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O_connection_url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O_connection_proxy:</td>\n",
       "<td>{\"http\": null, \"https\": null}</td></tr>\n",
       "<tr><td>H2O_internal_security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>Python_version:</td>\n",
       "<td>3.10.11 final</td></tr></tbody>\n",
       "  </table>\n",
       "</div>\n"
      ],
      "text/plain": [
       "--------------------------  -------------------------------\n",
       "H2O_cluster_uptime:         05 secs\n",
       "H2O_cluster_timezone:       Australia/Brisbane\n",
       "H2O_data_parsing_timezone:  UTC\n",
       "H2O_cluster_version:        3.44.0.3\n",
       "H2O_cluster_version_age:    1 month and 8 days\n",
       "H2O_cluster_name:           H2O_from_python_s5236256_yvrl5q\n",
       "H2O_cluster_total_nodes:    1\n",
       "H2O_cluster_free_memory:    3.922 Gb\n",
       "H2O_cluster_total_cores:    0\n",
       "H2O_cluster_allowed_cores:  0\n",
       "H2O_cluster_status:         locked, healthy\n",
       "H2O_connection_url:         http://127.0.0.1:54321\n",
       "H2O_connection_proxy:       {\"http\": null, \"https\": null}\n",
       "H2O_internal_security:      False\n",
       "Python_version:             3.10.11 final\n",
       "--------------------------  -------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n",
      "AutoML progress: |\n",
      "20:46:29.884: AutoML: XGBoost is not available; skipping it.\n",
      "20:46:29.977: _nfolds param, nfolds cannot be larger than the number of rows (4).\n",
      "20:46:29.978: _train param, Dropping bad and constant columns: [ant2_std, ant1_count, ant2_count, ant4_count, ant1_mean, ant4_std, ant2_mean, ant4_mean, ant1_std]\n",
      "20:46:29.994: _nfolds param, nfolds cannot be larger than the number of rows (4).\n",
      "20:46:29.994: _train param, Dropping bad and constant columns: [ant2_std, ant1_count, ant2_count, ant4_count, ant1_mean, ant4_std, ant2_mean, ant4_mean, ant1_std]\n",
      "20:46:29.995: _min_rows param, The dataset size is too small to split for min_rows=100.0: must have at least 200.0 (weighted) rows, but have only 4.0.\n",
      "20:46:29.999: _nfolds param, nfolds cannot be larger than the number of rows (4).\n",
      "20:46:29.999: _train param, Dropping bad and constant columns: [ant2_std, ant1_count, ant2_count, ant4_count, ant1_mean, ant4_std, ant2_mean, ant4_mean, ant1_std]\n",
      "20:46:29.999: _nfolds param, nfolds cannot be larger than the number of rows (4).\n",
      "20:46:29.999: _train param, Dropping bad and constant columns: [ant2_std, ant1_count, ant2_count, ant4_count, ant1_mean, ant4_std, ant2_mean, ant4_mean, ant1_std]\n",
      "20:46:29.999: _min_rows param, The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 4.0.\n",
      "20:46:30.15: _nfolds param, nfolds cannot be larger than the number of rows (4).\n",
      "20:46:30.15: _train param, Dropping bad and constant columns: [ant2_std, ant1_count, ant2_count, ant4_count, ant1_mean, ant4_std, ant2_mean, ant4_mean, ant1_std]\n",
      "20:46:30.15: _min_rows param, The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 4.0.\n",
      "20:46:30.29: _nfolds param, nfolds cannot be larger than the number of rows (4).\n",
      "20:46:30.29: _train param, Dropping bad and constant columns: [ant2_std, ant1_count, ant2_count, ant4_count, ant1_mean, ant4_std, ant2_mean, ant4_mean, ant1_std]\n",
      "20:46:30.29: _min_rows param, The dataset size is too small to split for min_rows=10.0: must have at least 20.0 (weighted) rows, but have only 4.0.\n",
      "20:46:30.36: _nfolds param, nfolds cannot be larger than the number of rows (4).\n",
      "20:46:30.36: _train param, Dropping bad and constant columns: [ant2_std, ant1_count, ant2_count, ant4_count, ant1_mean, ant4_std, ant2_mean, ant4_mean, ant1_std]\n",
      "20:46:30.36: _nfolds param, nfolds cannot be larger than the number of rows (4).\n",
      "20:46:30.36: _train param, Dropping bad and constant columns: [ant2_std, ant1_count, ant2_count, ant4_count, ant1_mean, ant4_std, ant2_mean, ant4_mean, ant1_std]\n",
      "20:46:30.52: _nfolds param, nfolds cannot be larger than the number of rows (4).\n",
      "20:46:30.52: _train param, Dropping bad and constant columns: [ant2_std, ant1_count, ant2_count, ant4_count, ant1_mean, ant4_std, ant2_mean, ant4_mean, ant1_std]\n",
      "20:46:30.164: GBM_grid_1_AutoML_1_20240128_204629 [GBM Grid Search] failed: java.lang.AssertionError\n",
      "20:46:30.180: Empty leaderboard.\n",
      "AutoML was not able to build any model within a max runtime constraint of 3600 seconds, you may want to increase this value before retrying.\n",
      "\n",
      " (failed)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Job with key $03017f00000132d4ffffffff$_abececde805a180d85e5a83b258446b3 failed with an exception: water.exceptions.H2OAutoMLException: Aborting AutoML after too many consecutive model failures\nstacktrace: \nwater.exceptions.H2OAutoMLException: Aborting AutoML after too many consecutive model failures\r\n\tat ai.h2o.automl.AutoML.learn(AutoML.java:776)\r\n\tat ai.h2o.automl.AutoML.run(AutoML.java:494)\r\n\tat ai.h2o.automl.H2OJob$1.compute2(H2OJob.java:33)\r\n\tat water.H2O$H2OCountedCompleter.compute(H2O.java:1689)\r\n\tat jsr166y.CountedCompleter.exec(CountedCompleter.java:468)\r\n\tat jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)\r\n\tat jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:976)\r\n\tat jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)\r\n\tat jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m train \u001b[38;5;241m=\u001b[39m h2o\u001b[38;5;241m.\u001b[39mH2OFrame(training_input)\n\u001b[0;32m     31\u001b[0m aml \u001b[38;5;241m=\u001b[39m H2OAutoML(max_models\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, seed\u001b[38;5;241m=\u001b[39mseed, stopping_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m, sort_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m, max_runtime_secs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3600\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m \u001b[43maml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictors_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdimension\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(aml\u001b[38;5;241m.\u001b[39mleaderboard)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Save the leader model\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# h2o.save_model(aml.leader, path = f\"Example_data\\Output\\Trained_models\\{dimension}\", force=True)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Make predictions on the test data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\s5236256\\Documents\\GitHub\\ml4rt\\.venv\\lib\\site-packages\\h2o\\automl\\_estimator.py:682\u001b[0m, in \u001b[0;36mH2OAutoML.train\u001b[1;34m(self, x, y, training_frame, fold_column, weights_column, validation_frame, leaderboard_frame, blending_frame)\u001b[0m\n\u001b[0;32m    680\u001b[0m poll_updates \u001b[38;5;241m=\u001b[39m ft\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll_training_updates, verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbosity, state\u001b[38;5;241m=\u001b[39m{})\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoll_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpoll_updates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    684\u001b[0m     poll_updates(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_job, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\s5236256\\Documents\\GitHub\\ml4rt\\.venv\\lib\\site-packages\\h2o\\job.py:88\u001b[0m, in \u001b[0;36mH2OJob.poll\u001b[1;34m(self, poll_updates)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFAILED\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob, \u001b[38;5;28mdict\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstacktrace\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob)):\n\u001b[1;32m---> 88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob with key \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m failed with an exception: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mstacktrace: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m                                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstacktrace\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob with key \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m failed with an exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjob_key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception))\n",
      "\u001b[1;31mOSError\u001b[0m: Job with key $03017f00000132d4ffffffff$_abececde805a180d85e5a83b258446b3 failed with an exception: water.exceptions.H2OAutoMLException: Aborting AutoML after too many consecutive model failures\nstacktrace: \nwater.exceptions.H2OAutoMLException: Aborting AutoML after too many consecutive model failures\r\n\tat ai.h2o.automl.AutoML.learn(AutoML.java:776)\r\n\tat ai.h2o.automl.AutoML.run(AutoML.java:494)\r\n\tat ai.h2o.automl.H2OJob$1.compute2(H2OJob.java:33)\r\n\tat water.H2O$H2OCountedCompleter.compute(H2O.java:1689)\r\n\tat jsr166y.CountedCompleter.exec(CountedCompleter.java:468)\r\n\tat jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)\r\n\tat jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:976)\r\n\tat jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1479)\r\n\tat jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\r\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the training and testing data\n",
    "train_data_preproc, predictors_train = preprocess_sim_data(train_data, freq, tower_locs, routine)\n",
    "test_data_preproc, predictors_test = preprocess_sim_data(test_data, freq, tower_locs, routine)\n",
    "\n",
    "unique_ids = train_data_preproc['Point_ID'].unique().tolist()\n",
    "\n",
    "# Initialise h2o\n",
    "h2o.init(nthreads = 2)\n",
    "\n",
    "# For loop to sample training data, train, and test the models and export the result\n",
    "train_data_length = len(unique_ids)\n",
    "n = 5\n",
    "run = 1\n",
    "\n",
    "while n <= train_data_length:\n",
    "    # Create a random subset of the training dataframe based on random Point_IDs, with the number of points equal to n\n",
    "    random.shuffle(unique_ids)\n",
    "    subset = unique_ids[:n]\n",
    "    train_data_preproc_sample = train_data_preproc[train_data_preproc['Point_ID'].isin(subset)]\n",
    "\n",
    "    test_data_preproc_n = test_data_preproc.copy()\n",
    "    results = []\n",
    "\n",
    "    # Train, save and test the models for each dimension\n",
    "    for dimension in dimensions:\n",
    "        # print(f\"Training model for {dimension}\")\n",
    "        # Train the model\n",
    "        variables = predictors_train + [dimension]\n",
    "        training_input = train_data_preproc_sample[variables]\n",
    "        train = h2o.H2OFrame(training_input)\n",
    "        aml = H2OAutoML(max_models=20, seed=seed, stopping_metric='MAE', sort_metric='MAE', max_runtime_secs=600)\n",
    "        aml.train(x=predictors_train, y=dimension, training_frame=train)\n",
    "        print(aml.leaderboard)\n",
    "\n",
    "        # Save the leader model\n",
    "        # h2o.save_model(aml.leader, path = f\"Example_data\\Output\\Trained_models\\{dimension}\", force=True)\n",
    "\n",
    "        # Make predictions on the test data\n",
    "        test = h2o.H2OFrame(test_data_preproc_n)\n",
    "        preds = aml.leader.predict(test)\n",
    "\n",
    "        # Save predictions to a new column in the test dataframe\n",
    "        pred_column_name = f\"{dimension}_pred\"\n",
    "        test_data_preproc_n[pred_column_name] = preds.as_data_frame()\n",
    "\n",
    "    # Post process the test predictions to calculate location from the radio tower locations\n",
    "    test_predictions_tower = postprocess_data(test_data_preproc_n, tower_locs)\n",
    "\n",
    "    # Location averaging functions\n",
    "    test_location_estimates = location_averaging(test_predictions_tower)\n",
    "    test_location_estimates = calculate_error(test_location_estimates)\n",
    "\n",
    "    # Calculate the mean absolute error of UTM_predictions['distance'] and the standard error\n",
    "    mean_error = np.mean(test_location_estimates['error_m'])\n",
    "    std_error = stats.sem(test_location_estimates['error_m'])\n",
    "\n",
    "    results.append({'r': run, 'n': n, 'mean_error': mean_error, 'std_error': std_error})\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(f'SBTF_data/paper_results/sample_size_trial/individual_runs/run_{run}_n{n}_error.csv', index=False)\n",
    "\n",
    "    print(f'Mean error (+/-SE) = {mean_error} (+/- {std_error})')\n",
    "    \n",
    "    n = n+1\n",
    "\n",
    "# Stop h2o\n",
    "h2o.cluster().shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_tower.to_excel(\"test_tower_predictions.xlsx\", index=False)\n",
    "test_location_estimates.to_excel(\"UTM_predictions.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_df = test_location_estimates.sort_values(by='error_m', ascending=False)\n",
    "# sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_preproc.to_excel(\"train_data_preproc.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c9c44150708f88dcc61b6a40c517040b165081f9891458623fe805b4ae9321d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
