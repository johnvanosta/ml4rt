{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In prep\n",
    "import pandas as pd\n",
    "import utm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuitCV, ElasticNetCV, BayesianRidge, HuberRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "seed = 38 # Seed for train/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data paths\n",
    "train_data = 'Example_data\\Output\\Train_test_data\\Training_Tag_GPS_locations.xlsx'\n",
    "test_data = 'Example_data\\Output\\Train_test_data\\Testing_Tag_GPS_locations.xlsx'\n",
    "radio_tower_xy_path = 'H:\\My Drive\\Colab Notebooks\\RadioTelemetry\\Tower_data\\RTEastNorth.xlsx'\n",
    "\n",
    "# Variable parameters\n",
    "frequencies = ['1min'] # Add more for final run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Example_data\\\\Output\\\\Train_test_data\\\\Training_Tag_GPS_locations.xlsx'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to convert geographic to projected coordinates\n",
    "def from_latlon(lat, lon):\n",
    "    easting, northing, zone_num, zone_letter = utm.from_latlon(lat, lon)\n",
    "    return easting, northing, zone_num, zone_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sim_data(sim_data, data_type, freq, tower_locs):\n",
    "    # Get data\n",
    "    sim_dat_filt = sim_data[sim_data['Data_type'].isin(data_type)]\n",
    "    \n",
    "    # make column with the datetime to nearest 'freq' value (e.g. 5min)\n",
    "    sim_dat_filt = sim_dat_filt.assign(DateTime = sim_dat_filt['DateAndTime'].dt.floor(freq=freq))\n",
    "  \n",
    "    # group by datetime, tag, tower and antenna, compute mean power and std power, pivot to antennas\n",
    "    sim_dat_filt = (\n",
    "            sim_dat_filt.groupby(['DateTime', 'TowerID', 'TagID', 'Antenna', 'POINT_X', 'POINT_Y'])['Power']\n",
    "            .agg(['mean', 'count', np.std])\n",
    "            .reset_index()\n",
    "            .pivot_table(index=['DateTime', 'TowerID', 'TagID', 'POINT_X', 'POINT_Y'], columns='Antenna', values=['mean', 'count', 'std'])\n",
    "            .reset_index()\n",
    "        )\n",
    "    \n",
    "    # Rename columns\n",
    "    sim_dat_filt.columns = [f\"{col[0]}{col[1]}\" if col[1] != \"\" else col[0] for col in sim_dat_filt.columns.values]\n",
    "    sim_dat_filt = sim_dat_filt.rename(columns={ 'mean1': 'ant1_mean', 'mean2': 'ant2_mean', 'mean3': 'ant3_mean', 'mean4': 'ant4_mean',\n",
    "                                                  'count1': 'ant1_count', 'count2': 'ant2_count', 'count3': 'ant3_count', 'count4': 'ant4_count',\n",
    "                                                  'std1': 'ant1_std', 'std2': 'ant2_std', 'std3': 'ant3_std', 'std4': 'ant4_std'})\n",
    "    \n",
    "    # Calculate the mean std and total count across the antennas\n",
    "    sim_dat_filt['mean_std'] = sim_dat_filt[['ant1_std', 'ant2_std', 'ant3_std', 'ant4_std']].mean(axis=1)\n",
    "    sim_dat_filt['total_count'] = sim_dat_filt[['ant1_count', 'ant2_count', 'ant3_count', 'ant4_count']].sum(axis=1)\n",
    "\n",
    "    # Fill missing values with 0\n",
    "    sim_dat_filt = sim_dat_filt.fillna(value=0)\n",
    "     \n",
    "    # Calculate easting and northing from lat long\n",
    "    sim_dat_filt['easting'], sim_dat_filt['northing'], sim_dat_filt['zone_num'], sim_dat_filt['zone_letter'] = from_latlon(sim_dat_filt['POINT_Y'].values, sim_dat_filt['POINT_X'].values)\n",
    "\n",
    "    # Create a dictionary of the coordinates of the towers\n",
    "    offset_dict = tower_locs.set_index('TowerID').to_dict()\n",
    "    point_x = offset_dict['POINT_X']\n",
    "    point_y = offset_dict['POINT_Y']\n",
    "\n",
    "    # Standardise the coordinates so that the tower location == 0 on both the x and y axes.\n",
    "    sim_dat_filt['xOffset'] = sim_dat_filt['easting'] - sim_dat_filt['TowerID'].map(point_x).fillna(0)\n",
    "    sim_dat_filt['yOffset'] = sim_dat_filt['northing'] - sim_dat_filt['TowerID'].map(point_y).fillna(0)\n",
    "    \n",
    "    return sim_dat_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert locations predictions back to easting northings\n",
    "\n",
    "def postprocess_data(prediction_data, tower_locs):\n",
    "    # Create a dictionary of the coordinates of the towers\n",
    "    offset_dict = tower_locs.set_index('TowerID').to_dict()\n",
    "    point_x = offset_dict['POINT_X']\n",
    "    point_y = offset_dict['POINT_Y']\n",
    "\n",
    "    # Change predicted x/y offset values to their respective easting/northing considering the location of the tower\n",
    "    prediction_data['easting_pred'] = prediction_data['xOffset_pred'] + prediction_data['TowerID'].map(point_x).fillna(0)\n",
    "    prediction_data['northing_pred'] = prediction_data['yOffset_pred'] + prediction_data['TowerID'].map(point_y).fillna(0)\n",
    "    \n",
    "    return prediction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data\n",
    "train_data = pd.read_excel(train_data)\n",
    "train_data['DateAndTime'] = pd.to_datetime(train_data['DateAndTime'])\n",
    "\n",
    "# Get testing data\n",
    "test_data = pd.read_excel(test_data)\n",
    "test_data['DateAndTime'] = pd.to_datetime(test_data['DateAndTime'])\n",
    "\n",
    "# Get tower locations\n",
    "tower_locs = pd.read_excel(radio_tower_xy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pre-processing parameters to be used\n",
    "\n",
    "# Fixed parameters\n",
    "data_type = ['Simulated BTFS', 'BTFS'] # Simulation or Live BTF, or could do both\n",
    "dimensions = ['xOffset', 'yOffset']\n",
    "predictors = ['ant1_mean', 'ant2_mean', 'ant3_mean', 'ant4_mean', 'ant1_count', 'ant2_count', 'ant3_count', 'ant4_count', 'ant1_std', 'ant2_std', 'ant3_std', 'ant4_std', 'mean_std', 'total_count']\n",
    "responses = ['xOffset', 'yOffset']\n",
    "scoring = 'neg_mean_absolute_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 11/42 [00:01<00:04,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GammaRegressor model failed to execute\n",
      "Some value(s) of y are out of the valid range of the loss 'HalfGammaLoss'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 29/42 [00:05<00:02,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoissonRegressor model failed to execute\n",
      "Some value(s) of y are out of the valid range of the loss 'HalfPoissonLoss'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:48<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Adjusted R-Squared  R-Squared      RMSE  \\\n",
      "Model                                                                    \n",
      "MLPRegressor                                 0.02       0.06    401.24   \n",
      "LinearSVR                                    0.01       0.05    404.25   \n",
      "SGDRegressor                                 0.01       0.05    404.27   \n",
      "GradientBoostingRegressor                    0.01       0.05    404.29   \n",
      "KernelRidge                                  0.01       0.05    404.36   \n",
      "HuberRegressor                               0.01       0.05    404.38   \n",
      "ElasticNetCV                                 0.01       0.04    404.77   \n",
      "RidgeCV                                      0.00       0.04    404.84   \n",
      "BayesianRidge                                0.00       0.04    404.90   \n",
      "Ridge                                        0.00       0.04    404.90   \n",
      "TransformedTargetRegressor                   0.00       0.04    404.91   \n",
      "LinearRegression                             0.00       0.04    404.91   \n",
      "Lars                                         0.00       0.04    404.91   \n",
      "AdaBoostRegressor                            0.00       0.04    405.04   \n",
      "LassoLarsIC                                  0.00       0.04    405.05   \n",
      "LassoCV                                      0.00       0.04    405.05   \n",
      "LassoLarsCV                                  0.00       0.04    405.05   \n",
      "LarsCV                                       0.00       0.04    405.05   \n",
      "LassoLars                                    0.00       0.04    405.06   \n",
      "Lasso                                        0.00       0.04    405.06   \n",
      "PassiveAggressiveRegressor                   0.00       0.04    405.23   \n",
      "OrthogonalMatchingPursuitCV                  0.00       0.04    405.30   \n",
      "ElasticNet                                  -0.00       0.04    406.34   \n",
      "KNeighborsRegressor                         -0.01       0.03    407.73   \n",
      "TweedieRegressor                            -0.01       0.03    407.88   \n",
      "OrthogonalMatchingPursuit                   -0.02       0.02    410.57   \n",
      "SVR                                         -0.03       0.01    412.24   \n",
      "NuSVR                                       -0.04       0.00    413.21   \n",
      "LGBMRegressor                               -0.04       0.00    413.21   \n",
      "QuantileRegressor                           -0.04      -0.00    414.41   \n",
      "HistGradientBoostingRegressor               -0.04      -0.00    414.68   \n",
      "DummyRegressor                              -0.04      -0.00    414.69   \n",
      "RandomForestRegressor                       -0.05      -0.01    415.69   \n",
      "BaggingRegressor                            -0.13      -0.08    430.74   \n",
      "ExtraTreesRegressor                         -0.15      -0.10    434.90   \n",
      "XGBRegressor                                -0.16      -0.11    436.38   \n",
      "RANSACRegressor                             -0.27      -0.22    456.87   \n",
      "ExtraTreeRegressor                          -0.30      -0.25    462.75   \n",
      "DecisionTreeRegressor                       -0.38      -0.33    477.34   \n",
      "GaussianProcessRegressor                -94424.46  -90721.50 124703.98   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "MLPRegressor                         1.51  \n",
      "LinearSVR                            0.06  \n",
      "SGDRegressor                         0.02  \n",
      "GradientBoostingRegressor            0.42  \n",
      "KernelRidge                          0.05  \n",
      "HuberRegressor                       0.03  \n",
      "ElasticNetCV                         0.18  \n",
      "RidgeCV                              0.01  \n",
      "BayesianRidge                        0.02  \n",
      "Ridge                                0.01  \n",
      "TransformedTargetRegressor           0.03  \n",
      "LinearRegression                     0.05  \n",
      "Lars                                 0.02  \n",
      "AdaBoostRegressor                    0.08  \n",
      "LassoLarsIC                          0.07  \n",
      "LassoCV                              0.14  \n",
      "LassoLarsCV                          0.11  \n",
      "LarsCV                               0.03  \n",
      "LassoLars                            0.04  \n",
      "Lasso                                0.01  \n",
      "PassiveAggressiveRegressor           0.02  \n",
      "OrthogonalMatchingPursuitCV          0.03  \n",
      "ElasticNet                           0.01  \n",
      "KNeighborsRegressor                  0.03  \n",
      "TweedieRegressor                     0.04  \n",
      "OrthogonalMatchingPursuit            0.01  \n",
      "SVR                                  0.16  \n",
      "NuSVR                                0.08  \n",
      "LGBMRegressor                        0.10  \n",
      "QuantileRegressor                   40.89  \n",
      "HistGradientBoostingRegressor        0.93  \n",
      "DummyRegressor                       0.01  \n",
      "RandomForestRegressor                1.29  \n",
      "BaggingRegressor                     0.12  \n",
      "ExtraTreesRegressor                  0.77  \n",
      "XGBRegressor                         0.18  \n",
      "RANSACRegressor                      0.41  \n",
      "ExtraTreeRegressor                   0.05  \n",
      "DecisionTreeRegressor                0.03  \n",
      "GaussianProcessRegressor             0.14  \n",
      "Best model for xOffset is MLPRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 9/42 [00:01<00:04,  7.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GammaRegressor model failed to execute\n",
      "Some value(s) of y are out of the valid range of the loss 'HalfGammaLoss'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 30/42 [00:04<00:02,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoissonRegressor model failed to execute\n",
      "Some value(s) of y are out of the valid range of the loss 'HalfPoissonLoss'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42/42 [00:35<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Adjusted R-Squared  R-Squared      RMSE  \\\n",
      "Model                                                                    \n",
      "GradientBoostingRegressor                    0.10       0.13    489.37   \n",
      "AdaBoostRegressor                            0.10       0.13    489.68   \n",
      "LarsCV                                       0.10       0.13    490.40   \n",
      "LassoLarsCV                                  0.10       0.13    490.40   \n",
      "LassoCV                                      0.10       0.13    490.40   \n",
      "LassoLars                                    0.10       0.13    490.46   \n",
      "Lasso                                        0.10       0.13    490.46   \n",
      "RidgeCV                                      0.09       0.13    490.50   \n",
      "Ridge                                        0.09       0.13    490.51   \n",
      "Lars                                         0.09       0.13    490.51   \n",
      "LinearRegression                             0.09       0.13    490.51   \n",
      "TransformedTargetRegressor                   0.09       0.13    490.51   \n",
      "KernelRidge                                  0.09       0.13    490.52   \n",
      "LassoLarsIC                                  0.09       0.13    490.52   \n",
      "BayesianRidge                                0.09       0.13    490.58   \n",
      "SGDRegressor                                 0.09       0.13    490.67   \n",
      "MLPRegressor                                 0.09       0.13    490.82   \n",
      "HuberRegressor                               0.09       0.13    490.91   \n",
      "ElasticNetCV                                 0.09       0.13    490.93   \n",
      "OrthogonalMatchingPursuitCV                  0.09       0.13    491.49   \n",
      "LinearSVR                                    0.09       0.12    492.70   \n",
      "PassiveAggressiveRegressor                   0.08       0.12    493.90   \n",
      "ElasticNet                                   0.08       0.12    494.63   \n",
      "HistGradientBoostingRegressor                0.07       0.11    496.37   \n",
      "LGBMRegressor                                0.07       0.10    497.73   \n",
      "TweedieRegressor                             0.06       0.10    499.16   \n",
      "RandomForestRegressor                        0.05       0.09    502.67   \n",
      "XGBRegressor                                 0.03       0.07    508.15   \n",
      "BaggingRegressor                             0.03       0.07    508.59   \n",
      "DecisionTreeRegressor                        0.02       0.05    511.57   \n",
      "KNeighborsRegressor                          0.00       0.04    514.79   \n",
      "SVR                                         -0.01       0.03    517.21   \n",
      "OrthogonalMatchingPursuit                   -0.01       0.03    517.53   \n",
      "NuSVR                                       -0.01       0.03    518.58   \n",
      "DummyRegressor                              -0.04      -0.00    526.09   \n",
      "QuantileRegressor                           -0.04      -0.00    526.12   \n",
      "ExtraTreesRegressor                         -0.18      -0.13    559.98   \n",
      "ExtraTreeRegressor                          -0.71      -0.64    673.93   \n",
      "RANSACRegressor                           -290.86    -279.42   8808.32   \n",
      "GaussianProcessRegressor                -74855.06  -71919.53 141064.31   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "GradientBoostingRegressor            0.40  \n",
      "AdaBoostRegressor                    0.08  \n",
      "LarsCV                               0.03  \n",
      "LassoLarsCV                          0.09  \n",
      "LassoCV                              0.15  \n",
      "LassoLars                            0.05  \n",
      "Lasso                                0.01  \n",
      "RidgeCV                              0.02  \n",
      "Ridge                                0.03  \n",
      "Lars                                 0.01  \n",
      "LinearRegression                     0.04  \n",
      "TransformedTargetRegressor           0.03  \n",
      "KernelRidge                          0.04  \n",
      "LassoLarsIC                          0.06  \n",
      "BayesianRidge                        0.03  \n",
      "SGDRegressor                         0.02  \n",
      "MLPRegressor                         1.93  \n",
      "HuberRegressor                       0.02  \n",
      "ElasticNetCV                         0.21  \n",
      "OrthogonalMatchingPursuitCV          0.02  \n",
      "LinearSVR                            0.04  \n",
      "PassiveAggressiveRegressor           0.01  \n",
      "ElasticNet                           0.05  \n",
      "HistGradientBoostingRegressor        0.73  \n",
      "LGBMRegressor                        0.19  \n",
      "TweedieRegressor                     0.02  \n",
      "RandomForestRegressor                1.04  \n",
      "XGBRegressor                         0.20  \n",
      "BaggingRegressor                     0.10  \n",
      "DecisionTreeRegressor                0.02  \n",
      "KNeighborsRegressor                  0.02  \n",
      "SVR                                  0.14  \n",
      "OrthogonalMatchingPursuit            0.01  \n",
      "NuSVR                                0.06  \n",
      "DummyRegressor                       0.03  \n",
      "QuantileRegressor                   28.24  \n",
      "ExtraTreesRegressor                  0.49  \n",
      "ExtraTreeRegressor                   0.02  \n",
      "RANSACRegressor                      0.24  \n",
      "GaussianProcessRegressor             0.19  \n",
      "Best model for yOffset is GradientBoostingRegressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initial pass over the data using lazy predict to identify possible options\n",
    "freq = '5min'\n",
    "\n",
    "for dimension in dimensions:\n",
    "    sim_data_preproc = preprocess_sim_data(train_data, data_type, freq, tower_locs)\n",
    "    X_train = sim_data_preproc[predictors]\n",
    "    y_train = sim_data_preproc[dimension] # Will need to adjust this to iterate over x and y xOffset\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=seed)\n",
    "\n",
    "    reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n",
    "    models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
    "    print(models)\n",
    "    best_model = models[:1]\n",
    "    best_model_name = best_model.index[0]\n",
    "    print(f\"Best model for {dimension} is {best_model_name}\")\n",
    "    # # best_model.index[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "if best_model_name == 'AdaBoostRegressor':\n",
    "    model = AdaBoostRegressor()\n",
    "elif best_model_name == 'GradientBoostingRegressor':\n",
    "    model = GradientBoostingRegressor()\n",
    "elif best_model_name == 'MLPRegressor':\n",
    "    model = MLPRegressor()\n",
    "elif best_model_name == 'OrthogonalMatchingPursuitCV':\n",
    "    model = OrthogonalMatchingPursuitCV()\n",
    "elif best_model_name == 'ElasticNetCV':\n",
    "    model = ElasticNetCV()\n",
    "elif best_model_name == 'BayesianRidge':\n",
    "    model = BayesianRidge()\n",
    "elif best_model_name == 'HuberRegressor':\n",
    "    model = HuberRegressor()\n",
    "else:\n",
    "    print('Model not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingRegressor\n",
      "Best model GradientBoostingRegressor not in list of models to be tested. Skipping grid search\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    AdaBoostRegressor(),\n",
    "    GradientBoostingRegressor,\n",
    "    MLPRegressor(),\n",
    "    OrthogonalMatchingPursuitCV(),\n",
    "    ElasticNetCV(),\n",
    "    BayesianRidge(),\n",
    "    HuberRegressor(),\n",
    "]\n",
    "\n",
    "#Check if best performing lazy predict model is in the list. If it is not then don't run grid search\n",
    "if best_model in models:\n",
    "    print(f\"Running grid search for {model}\")\n",
    "else:\n",
    "    print(f\"Best model {best_model.index[0]} not in list of models to be tested. Skipping grid search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model GradientBoostingRegressor not in list of models to be tested. Skipping grid search\n"
     ]
    }
   ],
   "source": [
    "# Take best model from lazy predict and run them with grid search to find best parameters\n",
    "# Set up the models to be tested\n",
    "models = [\n",
    "    AdaBoostRegressor(),\n",
    "    GradientBoostingRegressor(),\n",
    "    MLPRegressor(),\n",
    "    OrthogonalMatchingPursuitCV(),\n",
    "    ElasticNetCV(),\n",
    "    BayesianRidge(),\n",
    "    HuberRegressor(),\n",
    "]\n",
    "\n",
    "# Check if best performing lazy predict model is in the list. If it is not then don't run grid search\n",
    "if best_model_name in models:\n",
    "    print(f\"Running grid search for {best_model_name}\")\n",
    "else:\n",
    "    print(f\"Best model {best_model_name} not in list of models to be tested. Skipping grid search\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model                            Adjusted R-Squared  R-Squared   RMSE  Time Taken\n",
      "Model                                                                      \n",
      "GradientBoostingRegressor                0.10       0.13 489.37        0.29 not in list of models to be tested. Skipping grid search\n"
     ]
    }
   ],
   "source": [
    "# Take best model from lazy predict and run them with grid search to find best parameters\n",
    "# Set up the models to be tested\n",
    "models = [\n",
    "    AdaBoostRegressor(),\n",
    "    GradientBoostingRegressor(),\n",
    "    MLPRegressor(),\n",
    "    OrthogonalMatchingPursuitCV(),\n",
    "    ElasticNetCV(),\n",
    "    BayesianRidge(),\n",
    "    HuberRegressor(),\n",
    "]\n",
    "\n",
    "# Check if best performing lazy predict model is in the list. If it is not then don't run grid search\n",
    "best_model_instance = None\n",
    "for model in models:\n",
    "    if isinstance(best_model, type(model)):\n",
    "        best_model_instance = model\n",
    "        break\n",
    "\n",
    "if best_model_instance is not None:\n",
    "    print(f\"Running grid search for {best_model_instance}\")\n",
    "else:\n",
    "    print(f\"Best model {best_model} not in list of models to be tested. Skipping grid search\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AdaBoostRegressor(),\n",
       " GradientBoostingRegressor(),\n",
       " MLPRegressor(),\n",
       " OrthogonalMatchingPursuitCV(),\n",
       " ElasticNetCV(),\n",
       " BayesianRidge(),\n",
       " HuberRegressor()]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingRegressor()\n",
      "GradientBoostingRegressor()\n"
     ]
    }
   ],
   "source": [
    "print(f'{best_model_name}()')\n",
    "print('GradientBoostingRegressor()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Adjusted R-Squared</th>\n",
       "      <th>R-Squared</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.13</td>\n",
       "      <td>489.37</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Adjusted R-Squared  R-Squared   RMSE  Time Taken\n",
       "Model                                                                      \n",
       "GradientBoostingRegressor                0.10       0.13 489.37        0.29"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Take best model from lazy predict and run them with grid search to find best parameters\n",
    "# Set up the models to be tested\n",
    "models = [\n",
    "    AdaBoostRegressor(),\n",
    "    GradientBoostingRegressor(),\n",
    "    MLPRegressor(),\n",
    "    OrthogonalMatchingPursuitCV(),\n",
    "    ElasticNetCV(),\n",
    "    BayesianRidge(),\n",
    "    HuberRegressor(),\n",
    "]\n",
    "\n",
    "# Check if best performing lazy predict model is in the list. If it is not then don't run grid search\n",
    "if best_overall_model in models:\n",
    "    print(\"Best model from lazy predict is in the models list\")\n",
    "else:\n",
    "    print(\"Best model from lazy predict is not in the models list\")\n",
    "\n",
    "# Set up the parameters to be tested\n",
    "params = [\n",
    "    {'n_estimators': [10, 50, 100, 200, 500, 1000]},\n",
    "    {'n_estimators': [10, 50, 100, 200, 500, 1000]},\n",
    "    {'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "        'activation': ['tanh', 'relu'],\n",
    "        'solver': ['sgd', 'adam'],\n",
    "        'alpha': [0.0001, 0.05],\n",
    "        'learning_rate': ['constant','adaptive']},\n",
    "    {'fit_intercept': [True, False]},\n",
    "    {'l1_ratio': [0.1, 0.5, 0.9]},\n",
    "    {'alpha_1': [1e-6, 1e-5, 1e-4],\n",
    "        'alpha_2': [1e-6, 1e-5, 1e-4],\n",
    "        'lambda_1': [1e-6, 1e-5, 1e-4],\n",
    "        'lambda_2': [1e-6, 1e-5, 1e-4]},\n",
    "    {'epsilon': [1.35, 1.5, 1.75],\n",
    "        'max_iter': [100, 300, 500]}\n",
    "]\n",
    "\n",
    "# Enact hyperparameter tuning if best model from lazy predict is in the models list\n",
    "\n",
    "\n",
    "# Set up the grid search\n",
    "for dimension in dimensions:\n",
    "    sim_data_preproc = preprocess_sim_data(train_data, data_type, freq, tower_locs)\n",
    "    X_train = sim_data_preproc[predictors]\n",
    "    y_train = sim_data_preproc[dimension] # Will need to adjust this to iterate over x and y xOffset\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.3, random_state=seed)\n",
    "\n",
    "    for model, param in zip(models, params):\n",
    "        grid = GridSearchCV(estimator=model, param_grid=param, scoring=scoring, cv=5, n_jobs=-1)\n",
    "        grid_result = grid.fit(X_train, y_train)\n",
    "        # print(grid_result.best_params_)\n",
    "        # print(grid_result.best_score_)\n",
    "        # print(grid_result.best_estimator_)\n",
    "        # print(grid_result.cv_results_)\n",
    "        # print(grid_result.scorer_)\n",
    "        # print(grid_result.n_splits_)\n",
    "        # print(grid_result.refit_time_)\n",
    "        # print(grid_result.best_index_)\n",
    "        # print(grid_result.scoring)\n",
    "        # print(grid_result.verbose)\n",
    "        # print(grid_result.error_score)\n",
    "        # print(grid_result.return_train_score)\n",
    "        print(grid_result.multimetric_)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=HuberRegressor(), n_jobs=-1,\n",
       "             param_grid={&#x27;epsilon&#x27;: [1.35, 1.5, 1.75],\n",
       "                         &#x27;max_iter&#x27;: [100, 300, 500]},\n",
       "             scoring=&#x27;neg_mean_absolute_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=HuberRegressor(), n_jobs=-1,\n",
       "             param_grid={&#x27;epsilon&#x27;: [1.35, 1.5, 1.75],\n",
       "                         &#x27;max_iter&#x27;: [100, 300, 500]},\n",
       "             scoring=&#x27;neg_mean_absolute_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: HuberRegressor</label><div class=\"sk-toggleable__content\"><pre>HuberRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HuberRegressor</label><div class=\"sk-toggleable__content\"><pre>HuberRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=HuberRegressor(), n_jobs=-1,\n",
       "             param_grid={'epsilon': [1.35, 1.5, 1.75],\n",
       "                         'max_iter': [100, 300, 500]},\n",
       "             scoring='neg_mean_absolute_error')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for Ada boost regressor\n",
      "Best parameters for Ada boost regressor are: {'learning_rate': 0.01, 'loss': 'linear', 'n_estimators': 25}\n",
      "Optimizing hyperparameters for Gradient boost regressor\n",
      "Best parameters for Gradient boost regressor are: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 25}\n",
      "Optimizing hyperparameters for MLP regressor\n",
      "Best parameters for MLP regressor are: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50,)}\n",
      "Optimizing hyperparameters for Orthogonal Matching Pursuit CV\n",
      "Best parameters for Orthogonal Matching Pursuit CV are: {'cv': None, 'n_jobs': -1, 'normalize': True}\n",
      "Optimizing hyperparameters for Elastic Net CV\n",
      "Best parameters for Elastic Net CV are: {'cv': 10, 'eps': 1e-05, 'l1_ratio': 0.9, 'n_jobs': -1}\n",
      "Optimizing hyperparameters for Bayesian Ridge\n",
      "Best parameters for Bayesian Ridge are: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'fit_intercept': False, 'lambda_1': 1e-08, 'lambda_2': 1e-06, 'n_iter': 100, 'tol': 0.001}\n",
      "Optimizing hyperparameters for Huber Regressor\n",
      "Best parameters for Huber Regressor are: {'alpha': 0.001, 'epsilon': 1.35, 'fit_intercept': False, 'max_iter': 500, 'tol': 0.001, 'warm_start': True}\n",
      "Optimizing hyperparameters for Ada boost regressor\n",
      "Best parameters for Ada boost regressor are: {'learning_rate': 0.01, 'loss': 'exponential', 'n_estimators': 75}\n",
      "Optimizing hyperparameters for Gradient boost regressor\n",
      "Best parameters for Gradient boost regressor are: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 75}\n",
      "Optimizing hyperparameters for MLP regressor\n",
      "Best parameters for MLP regressor are: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50,)}\n",
      "Optimizing hyperparameters for Orthogonal Matching Pursuit CV\n",
      "Best parameters for Orthogonal Matching Pursuit CV are: {'cv': 3, 'n_jobs': -1, 'normalize': False}\n",
      "Optimizing hyperparameters for Elastic Net CV\n",
      "Best parameters for Elastic Net CV are: {'cv': 5, 'eps': 0.001, 'l1_ratio': 0.1, 'n_jobs': -1}\n",
      "Optimizing hyperparameters for Bayesian Ridge\n",
      "Best parameters for Bayesian Ridge are: {'alpha_1': 1e-06, 'alpha_2': 1e-06, 'fit_intercept': False, 'lambda_1': 1e-08, 'lambda_2': 1e-06, 'n_iter': 100, 'tol': 1e-05}\n",
      "Optimizing hyperparameters for Huber Regressor\n",
      "Best parameters for Huber Regressor are: {'alpha': 0.01, 'epsilon': 1.35, 'fit_intercept': False, 'max_iter': 200, 'tol': 0.001, 'warm_start': True}\n",
      "Optimizing hyperparameters for Ada boost regressor\n",
      "Best parameters for Ada boost regressor are: {'learning_rate': 0.05, 'loss': 'linear', 'n_estimators': 25}\n",
      "Optimizing hyperparameters for Gradient boost regressor\n",
      "Best parameters for Gradient boost regressor are: {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 50}\n",
      "Optimizing hyperparameters for MLP regressor\n",
      "Best parameters for MLP regressor are: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (150,)}\n",
      "Optimizing hyperparameters for Orthogonal Matching Pursuit CV\n",
      "Best parameters for Orthogonal Matching Pursuit CV are: {'cv': 3, 'n_jobs': -1, 'normalize': True}\n",
      "Optimizing hyperparameters for Elastic Net CV\n",
      "Best parameters for Elastic Net CV are: {'cv': 10, 'eps': 0.0001, 'l1_ratio': 0.1, 'n_jobs': -1}\n",
      "Optimizing hyperparameters for Bayesian Ridge\n",
      "Best parameters for Bayesian Ridge are: {'alpha_1': 1e-06, 'alpha_2': 1e-07, 'fit_intercept': True, 'lambda_1': 1e-08, 'lambda_2': 1e-06, 'n_iter': 100, 'tol': 1e-05}\n",
      "Optimizing hyperparameters for Huber Regressor\n",
      "Best parameters for Huber Regressor are: {'alpha': 0.0001, 'epsilon': 1.35, 'fit_intercept': False, 'max_iter': 100, 'tol': 0.001, 'warm_start': True}\n",
      "Optimizing hyperparameters for Ada boost regressor\n",
      "Best parameters for Ada boost regressor are: {'learning_rate': 0.01, 'loss': 'exponential', 'n_estimators': 25}\n",
      "Optimizing hyperparameters for Gradient boost regressor\n",
      "Best parameters for Gradient boost regressor are: {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 50}\n",
      "Optimizing hyperparameters for MLP regressor\n",
      "Best parameters for MLP regressor are: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (150,)}\n",
      "Optimizing hyperparameters for Orthogonal Matching Pursuit CV\n",
      "Best parameters for Orthogonal Matching Pursuit CV are: {'cv': None, 'n_jobs': -1, 'normalize': False}\n",
      "Optimizing hyperparameters for Elastic Net CV\n",
      "Best parameters for Elastic Net CV are: {'cv': 3, 'eps': 1e-05, 'l1_ratio': 0.1, 'n_jobs': -1}\n",
      "Optimizing hyperparameters for Bayesian Ridge\n",
      "Best parameters for Bayesian Ridge are: {'alpha_1': 1e-06, 'alpha_2': 1e-08, 'fit_intercept': False, 'lambda_1': 1e-08, 'lambda_2': 1e-06, 'n_iter': 100, 'tol': 0.001}\n",
      "Optimizing hyperparameters for Huber Regressor\n",
      "Best parameters for Huber Regressor are: {'alpha': 0.001, 'epsilon': 1.5, 'fit_intercept': False, 'max_iter': 100, 'tol': 0.001, 'warm_start': True}\n",
      "Optimizing hyperparameters for Ada boost regressor\n",
      "Best parameters for Ada boost regressor are: {'learning_rate': 0.01, 'loss': 'exponential', 'n_estimators': 25}\n",
      "Optimizing hyperparameters for Gradient boost regressor\n",
      "Best parameters for Gradient boost regressor are: {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 50}\n",
      "Optimizing hyperparameters for MLP regressor\n",
      "Best parameters for MLP regressor are: {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (150,)}\n",
      "Optimizing hyperparameters for Orthogonal Matching Pursuit CV\n",
      "Best parameters for Orthogonal Matching Pursuit CV are: {'cv': None, 'n_jobs': -1, 'normalize': True}\n",
      "Optimizing hyperparameters for Elastic Net CV\n",
      "Best parameters for Elastic Net CV are: {'cv': 3, 'eps': 0.001, 'l1_ratio': 0.5, 'n_jobs': -1}\n",
      "Optimizing hyperparameters for Bayesian Ridge\n",
      "Best parameters for Bayesian Ridge are: {'alpha_1': 1e-08, 'alpha_2': 1e-06, 'fit_intercept': False, 'lambda_1': 1e-06, 'lambda_2': 1e-08, 'n_iter': 100, 'tol': 0.001}\n",
      "Optimizing hyperparameters for Huber Regressor\n",
      "Best parameters for Huber Regressor are: {'alpha': 0.01, 'epsilon': 1.35, 'fit_intercept': False, 'max_iter': 100, 'tol': 0.001, 'warm_start': True}\n",
      "Optimizing hyperparameters for Ada boost regressor\n",
      "Best parameters for Ada boost regressor are: {'learning_rate': 0.01, 'loss': 'square', 'n_estimators': 25}\n",
      "Optimizing hyperparameters for Gradient boost regressor\n",
      "Best parameters for Gradient boost regressor are: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "Optimizing hyperparameters for MLP regressor\n",
      "Best parameters for MLP regressor are: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (100,)}\n",
      "Optimizing hyperparameters for Orthogonal Matching Pursuit CV\n",
      "Best parameters for Orthogonal Matching Pursuit CV are: {'cv': None, 'n_jobs': -1, 'normalize': True}\n",
      "Optimizing hyperparameters for Elastic Net CV\n",
      "Best parameters for Elastic Net CV are: {'cv': 10, 'eps': 0.001, 'l1_ratio': 0.5, 'n_jobs': -1}\n",
      "Optimizing hyperparameters for Bayesian Ridge\n",
      "Best parameters for Bayesian Ridge are: {'alpha_1': 1e-08, 'alpha_2': 1e-06, 'fit_intercept': False, 'lambda_1': 1e-06, 'lambda_2': 1e-08, 'n_iter': 100, 'tol': 0.001}\n",
      "Optimizing hyperparameters for Huber Regressor\n",
      "Best parameters for Huber Regressor are: {'alpha': 0.0001, 'epsilon': 1.35, 'fit_intercept': False, 'max_iter': 100, 'tol': 0.001, 'warm_start': True}\n"
     ]
    }
   ],
   "source": [
    "# #Perform grid search on each model\n",
    "# trained_models = []\n",
    "# for dimension in dimensions:\n",
    "#     for freq in frequencies:\n",
    "#         sim_data_preproc = preprocess_sim_data(train_data, data_type, freq, tower_locs)\n",
    "#         X_train = sim_data_preproc[predictors]\n",
    "#         y_train = sim_data_preproc[dimension] # Will need to adjust this to iterate over x and y xOffset\n",
    "        \n",
    "#         for modelclass in modelclasses:\n",
    "#             modelname = modelclass[0]\n",
    "#             model = modelclass[1]\n",
    "#             params = modelclass[2]\n",
    "                \n",
    "#             print(\"Optimizing hyperparameters for \" + modelname)\n",
    "#             grid_search = GridSearchCV(model(), params, scoring=scoring, cv=5)\n",
    "#             grid_search.fit(X_train, y_train)\n",
    "#             best_model = grid_search.best_estimator_\n",
    "#             best_params = grid_search.best_params_\n",
    "#             best_score = grid_search.best_score_ # New line to get the best score\n",
    "#             trained_models.append((dimension, modelname, best_model, best_params, freq, best_score))\n",
    "#             print(\"Best parameters for \" + modelname + \" are: \" + str(best_params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xOffset',\n",
       "  'Ada boost regressor',\n",
       "  AdaBoostRegressor(learning_rate=0.01, n_estimators=25),\n",
       "  {'learning_rate': 0.01, 'loss': 'linear', 'n_estimators': 25},\n",
       "  '1min',\n",
       "  -239.4021428709205),\n",
       " ('xOffset',\n",
       "  'Gradient boost regressor',\n",
       "  GradientBoostingRegressor(max_depth=4, n_estimators=25),\n",
       "  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 25},\n",
       "  '1min',\n",
       "  -238.214439939294),\n",
       " ('xOffset',\n",
       "  'MLP regressor',\n",
       "  MLPRegressor(alpha=0.001, hidden_layer_sizes=(50,)),\n",
       "  {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50,)},\n",
       "  '1min',\n",
       "  -237.3117805602382),\n",
       " ('xOffset',\n",
       "  'Orthogonal Matching Pursuit CV',\n",
       "  OrthogonalMatchingPursuitCV(n_jobs=-1, normalize=True),\n",
       "  {'cv': None, 'n_jobs': -1, 'normalize': True},\n",
       "  '1min',\n",
       "  -243.04954850532744),\n",
       " ('xOffset',\n",
       "  'Elastic Net CV',\n",
       "  ElasticNetCV(cv=10, eps=1e-05, l1_ratio=0.9, n_jobs=-1),\n",
       "  {'cv': 10, 'eps': 1e-05, 'l1_ratio': 0.9, 'n_jobs': -1},\n",
       "  '1min',\n",
       "  -242.5870308120037),\n",
       " ('xOffset',\n",
       "  'Bayesian Ridge',\n",
       "  BayesianRidge(fit_intercept=False, lambda_1=1e-08, n_iter=100),\n",
       "  {'alpha_1': 1e-06,\n",
       "   'alpha_2': 1e-06,\n",
       "   'fit_intercept': False,\n",
       "   'lambda_1': 1e-08,\n",
       "   'lambda_2': 1e-06,\n",
       "   'n_iter': 100,\n",
       "   'tol': 0.001},\n",
       "  '1min',\n",
       "  -241.67004550112316),\n",
       " ('xOffset',\n",
       "  'Huber Regressor',\n",
       "  HuberRegressor(alpha=0.001, fit_intercept=False, max_iter=500, tol=0.001,\n",
       "                 warm_start=True),\n",
       "  {'alpha': 0.001,\n",
       "   'epsilon': 1.35,\n",
       "   'fit_intercept': False,\n",
       "   'max_iter': 500,\n",
       "   'tol': 0.001,\n",
       "   'warm_start': True},\n",
       "  '1min',\n",
       "  -237.66868574203835),\n",
       " ('xOffset',\n",
       "  'Ada boost regressor',\n",
       "  AdaBoostRegressor(learning_rate=0.01, loss='exponential', n_estimators=75),\n",
       "  {'learning_rate': 0.01, 'loss': 'exponential', 'n_estimators': 75},\n",
       "  '5min',\n",
       "  -245.26557436765228),\n",
       " ('xOffset',\n",
       "  'Gradient boost regressor',\n",
       "  GradientBoostingRegressor(n_estimators=75),\n",
       "  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 75},\n",
       "  '5min',\n",
       "  -246.3068285146033),\n",
       " ('xOffset',\n",
       "  'MLP regressor',\n",
       "  MLPRegressor(hidden_layer_sizes=(50,)),\n",
       "  {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50,)},\n",
       "  '5min',\n",
       "  -239.14662297594083),\n",
       " ('xOffset',\n",
       "  'Orthogonal Matching Pursuit CV',\n",
       "  OrthogonalMatchingPursuitCV(cv=3, n_jobs=-1, normalize=False),\n",
       "  {'cv': 3, 'n_jobs': -1, 'normalize': False},\n",
       "  '5min',\n",
       "  -248.9902825267065),\n",
       " ('xOffset',\n",
       "  'Elastic Net CV',\n",
       "  ElasticNetCV(cv=5, l1_ratio=0.1, n_jobs=-1),\n",
       "  {'cv': 5, 'eps': 0.001, 'l1_ratio': 0.1, 'n_jobs': -1},\n",
       "  '5min',\n",
       "  -251.01691215379566),\n",
       " ('xOffset',\n",
       "  'Bayesian Ridge',\n",
       "  BayesianRidge(fit_intercept=False, lambda_1=1e-08, n_iter=100, tol=1e-05),\n",
       "  {'alpha_1': 1e-06,\n",
       "   'alpha_2': 1e-06,\n",
       "   'fit_intercept': False,\n",
       "   'lambda_1': 1e-08,\n",
       "   'lambda_2': 1e-06,\n",
       "   'n_iter': 100,\n",
       "   'tol': 1e-05},\n",
       "  '5min',\n",
       "  -250.09317785613567),\n",
       " ('xOffset',\n",
       "  'Huber Regressor',\n",
       "  HuberRegressor(alpha=0.01, fit_intercept=False, max_iter=200, tol=0.001,\n",
       "                 warm_start=True),\n",
       "  {'alpha': 0.01,\n",
       "   'epsilon': 1.35,\n",
       "   'fit_intercept': False,\n",
       "   'max_iter': 200,\n",
       "   'tol': 0.001,\n",
       "   'warm_start': True},\n",
       "  '5min',\n",
       "  -243.02276116664092),\n",
       " ('xOffset',\n",
       "  'Ada boost regressor',\n",
       "  AdaBoostRegressor(learning_rate=0.05, n_estimators=25),\n",
       "  {'learning_rate': 0.05, 'loss': 'linear', 'n_estimators': 25},\n",
       "  '10min',\n",
       "  -250.27127600530875),\n",
       " ('xOffset',\n",
       "  'Gradient boost regressor',\n",
       "  GradientBoostingRegressor(max_depth=4, n_estimators=50),\n",
       "  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 50},\n",
       "  '10min',\n",
       "  -247.75844162793274),\n",
       " ('xOffset',\n",
       "  'MLP regressor',\n",
       "  MLPRegressor(alpha=0.001, hidden_layer_sizes=(150,)),\n",
       "  {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (150,)},\n",
       "  '10min',\n",
       "  -242.7007445913012),\n",
       " ('xOffset',\n",
       "  'Orthogonal Matching Pursuit CV',\n",
       "  OrthogonalMatchingPursuitCV(cv=3, n_jobs=-1, normalize=True),\n",
       "  {'cv': 3, 'n_jobs': -1, 'normalize': True},\n",
       "  '10min',\n",
       "  -255.48803432470595),\n",
       " ('xOffset',\n",
       "  'Elastic Net CV',\n",
       "  ElasticNetCV(cv=10, eps=0.0001, l1_ratio=0.1, n_jobs=-1),\n",
       "  {'cv': 10, 'eps': 0.0001, 'l1_ratio': 0.1, 'n_jobs': -1},\n",
       "  '10min',\n",
       "  -257.97597464806756),\n",
       " ('xOffset',\n",
       "  'Bayesian Ridge',\n",
       "  BayesianRidge(alpha_2=1e-07, lambda_1=1e-08, n_iter=100, tol=1e-05),\n",
       "  {'alpha_1': 1e-06,\n",
       "   'alpha_2': 1e-07,\n",
       "   'fit_intercept': True,\n",
       "   'lambda_1': 1e-08,\n",
       "   'lambda_2': 1e-06,\n",
       "   'n_iter': 100,\n",
       "   'tol': 1e-05},\n",
       "  '10min',\n",
       "  -257.42348570760646),\n",
       " ('xOffset',\n",
       "  'Huber Regressor',\n",
       "  HuberRegressor(fit_intercept=False, tol=0.001, warm_start=True),\n",
       "  {'alpha': 0.0001,\n",
       "   'epsilon': 1.35,\n",
       "   'fit_intercept': False,\n",
       "   'max_iter': 100,\n",
       "   'tol': 0.001,\n",
       "   'warm_start': True},\n",
       "  '10min',\n",
       "  -248.57011147228303),\n",
       " ('yOffset',\n",
       "  'Ada boost regressor',\n",
       "  AdaBoostRegressor(learning_rate=0.01, loss='exponential', n_estimators=25),\n",
       "  {'learning_rate': 0.01, 'loss': 'exponential', 'n_estimators': 25},\n",
       "  '1min',\n",
       "  -178.86112361296628),\n",
       " ('yOffset',\n",
       "  'Gradient boost regressor',\n",
       "  GradientBoostingRegressor(learning_rate=0.05, max_depth=4, n_estimators=50),\n",
       "  {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 50},\n",
       "  '1min',\n",
       "  -171.3862557410394),\n",
       " ('yOffset',\n",
       "  'MLP regressor',\n",
       "  MLPRegressor(hidden_layer_sizes=(150,)),\n",
       "  {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (150,)},\n",
       "  '1min',\n",
       "  -175.1439332488817),\n",
       " ('yOffset',\n",
       "  'Orthogonal Matching Pursuit CV',\n",
       "  OrthogonalMatchingPursuitCV(n_jobs=-1, normalize=False),\n",
       "  {'cv': None, 'n_jobs': -1, 'normalize': False},\n",
       "  '1min',\n",
       "  -196.82823637081185),\n",
       " ('yOffset',\n",
       "  'Elastic Net CV',\n",
       "  ElasticNetCV(cv=3, eps=1e-05, l1_ratio=0.1, n_jobs=-1),\n",
       "  {'cv': 3, 'eps': 1e-05, 'l1_ratio': 0.1, 'n_jobs': -1},\n",
       "  '1min',\n",
       "  -195.89291537699344),\n",
       " ('yOffset',\n",
       "  'Bayesian Ridge',\n",
       "  BayesianRidge(alpha_2=1e-08, fit_intercept=False, lambda_1=1e-08, n_iter=100),\n",
       "  {'alpha_1': 1e-06,\n",
       "   'alpha_2': 1e-08,\n",
       "   'fit_intercept': False,\n",
       "   'lambda_1': 1e-08,\n",
       "   'lambda_2': 1e-06,\n",
       "   'n_iter': 100,\n",
       "   'tol': 0.001},\n",
       "  '1min',\n",
       "  -195.56667302156015),\n",
       " ('yOffset',\n",
       "  'Huber Regressor',\n",
       "  HuberRegressor(alpha=0.001, epsilon=1.5, fit_intercept=False, tol=0.001,\n",
       "                 warm_start=True),\n",
       "  {'alpha': 0.001,\n",
       "   'epsilon': 1.5,\n",
       "   'fit_intercept': False,\n",
       "   'max_iter': 100,\n",
       "   'tol': 0.001,\n",
       "   'warm_start': True},\n",
       "  '1min',\n",
       "  -194.60657019955676),\n",
       " ('yOffset',\n",
       "  'Ada boost regressor',\n",
       "  AdaBoostRegressor(learning_rate=0.01, loss='exponential', n_estimators=25),\n",
       "  {'learning_rate': 0.01, 'loss': 'exponential', 'n_estimators': 25},\n",
       "  '5min',\n",
       "  -169.58163145612093),\n",
       " ('yOffset',\n",
       "  'Gradient boost regressor',\n",
       "  GradientBoostingRegressor(learning_rate=0.05, max_depth=4, n_estimators=50),\n",
       "  {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 50},\n",
       "  '5min',\n",
       "  -167.52937801813175),\n",
       " ('yOffset',\n",
       "  'MLP regressor',\n",
       "  MLPRegressor(alpha=0.01, hidden_layer_sizes=(150,)),\n",
       "  {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (150,)},\n",
       "  '5min',\n",
       "  -171.51475556104296),\n",
       " ('yOffset',\n",
       "  'Orthogonal Matching Pursuit CV',\n",
       "  OrthogonalMatchingPursuitCV(n_jobs=-1, normalize=True),\n",
       "  {'cv': None, 'n_jobs': -1, 'normalize': True},\n",
       "  '5min',\n",
       "  -192.71561530025193),\n",
       " ('yOffset',\n",
       "  'Elastic Net CV',\n",
       "  ElasticNetCV(cv=3, n_jobs=-1),\n",
       "  {'cv': 3, 'eps': 0.001, 'l1_ratio': 0.5, 'n_jobs': -1},\n",
       "  '5min',\n",
       "  -192.97946969814117),\n",
       " ('yOffset',\n",
       "  'Bayesian Ridge',\n",
       "  BayesianRidge(alpha_1=1e-08, fit_intercept=False, lambda_2=1e-08, n_iter=100),\n",
       "  {'alpha_1': 1e-08,\n",
       "   'alpha_2': 1e-06,\n",
       "   'fit_intercept': False,\n",
       "   'lambda_1': 1e-06,\n",
       "   'lambda_2': 1e-08,\n",
       "   'n_iter': 100,\n",
       "   'tol': 0.001},\n",
       "  '5min',\n",
       "  -192.17325009535472),\n",
       " ('yOffset',\n",
       "  'Huber Regressor',\n",
       "  HuberRegressor(alpha=0.01, fit_intercept=False, tol=0.001, warm_start=True),\n",
       "  {'alpha': 0.01,\n",
       "   'epsilon': 1.35,\n",
       "   'fit_intercept': False,\n",
       "   'max_iter': 100,\n",
       "   'tol': 0.001,\n",
       "   'warm_start': True},\n",
       "  '5min',\n",
       "  -191.6971524794516),\n",
       " ('yOffset',\n",
       "  'Ada boost regressor',\n",
       "  AdaBoostRegressor(learning_rate=0.01, loss='square', n_estimators=25),\n",
       "  {'learning_rate': 0.01, 'loss': 'square', 'n_estimators': 25},\n",
       "  '10min',\n",
       "  -170.55740107343092),\n",
       " ('yOffset',\n",
       "  'Gradient boost regressor',\n",
       "  GradientBoostingRegressor(n_estimators=50),\n",
       "  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50},\n",
       "  '10min',\n",
       "  -169.0274737959306),\n",
       " ('yOffset',\n",
       "  'MLP regressor',\n",
       "  MLPRegressor(alpha=0.001),\n",
       "  {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (100,)},\n",
       "  '10min',\n",
       "  -169.42681421187012),\n",
       " ('yOffset',\n",
       "  'Orthogonal Matching Pursuit CV',\n",
       "  OrthogonalMatchingPursuitCV(n_jobs=-1, normalize=True),\n",
       "  {'cv': None, 'n_jobs': -1, 'normalize': True},\n",
       "  '10min',\n",
       "  -192.7057438254659),\n",
       " ('yOffset',\n",
       "  'Elastic Net CV',\n",
       "  ElasticNetCV(cv=10, n_jobs=-1),\n",
       "  {'cv': 10, 'eps': 0.001, 'l1_ratio': 0.5, 'n_jobs': -1},\n",
       "  '10min',\n",
       "  -193.88328942517376),\n",
       " ('yOffset',\n",
       "  'Bayesian Ridge',\n",
       "  BayesianRidge(alpha_1=1e-08, fit_intercept=False, lambda_2=1e-08, n_iter=100),\n",
       "  {'alpha_1': 1e-08,\n",
       "   'alpha_2': 1e-06,\n",
       "   'fit_intercept': False,\n",
       "   'lambda_1': 1e-06,\n",
       "   'lambda_2': 1e-08,\n",
       "   'n_iter': 100,\n",
       "   'tol': 0.001},\n",
       "  '10min',\n",
       "  -193.47628119293486),\n",
       " ('yOffset',\n",
       "  'Huber Regressor',\n",
       "  HuberRegressor(fit_intercept=False, tol=0.001, warm_start=True),\n",
       "  {'alpha': 0.0001,\n",
       "   'epsilon': 1.35,\n",
       "   'fit_intercept': False,\n",
       "   'max_iter': 100,\n",
       "   'tol': 0.001,\n",
       "   'warm_start': True},\n",
       "  '10min',\n",
       "  -192.04354750812155)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The best performing model for xOffset was ('xOffset', 'MLP regressor', MLPRegressor(alpha=0.001, hidden_layer_sizes=(50,)), {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50,)}, '1min', -237.3117805602382)\n",
      "\n",
      "The best performing model for yOffset was ('yOffset', 'Gradient boost regressor', GradientBoostingRegressor(learning_rate=0.05, max_depth=4, n_estimators=50), {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 50}, '5min', -167.52937801813175)\n"
     ]
    }
   ],
   "source": [
    "# Select the best model for each dimension based off of the best score\n",
    "best_models = []\n",
    "for dimension in dimensions:\n",
    "    filtered = [x for x in trained_models if x[0] == dimension]\n",
    "    filtered.sort(key=lambda x: x[-1], reverse=True)\n",
    "    best_model = filtered[0]\n",
    "    best_models.append(best_model)\n",
    "    print(f\"\\nThe best performing model for {dimension} was {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xOffset',\n",
       "  'MLP regressor',\n",
       "  MLPRegressor(alpha=0.001, hidden_layer_sizes=(50,)),\n",
       "  {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50,)},\n",
       "  '1min',\n",
       "  -237.3117805602382),\n",
       " ('yOffset',\n",
       "  'Gradient boost regressor',\n",
       "  GradientBoostingRegressor(learning_rate=0.05, max_depth=4, n_estimators=50),\n",
       "  {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 50},\n",
       "  '5min',\n",
       "  -167.52937801813175)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the best models on the test data\n",
    "\n",
    "# Choose frequency of the xOffset\n",
    "best_model = [x for x in best_models if x[0] == 'xOffset']\n",
    "freq = best_model[0][4]\n",
    "\n",
    "# Preprocess the test data\n",
    "sim_data_preproc = preprocess_sim_data(test_data, data_type, freq, tower_locs)\n",
    "\n",
    "for dimension in dimensions:\n",
    "    best_model = [x for x in best_models if x[0] == dimension]\n",
    "    model = best_model[0][2]\n",
    "    freq = best_model[0][4]\n",
    "\n",
    "    # Isolate predictor (x) and response (y) variables from test data\n",
    "    X_test = sim_data_preproc[predictors]\n",
    "    y_test = sim_data_preproc[dimension]\n",
    "\n",
    "    # Make predictions on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Save predictions to a new column in the test data pandas dataframe\n",
    "    pred_column_name = f\"{dimension}_pred\"\n",
    "    sim_data_preproc[pred_column_name] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error (+/-SE) = 229.63651248633454 (+/- 10.285663119389575)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of the predictions\n",
    "\n",
    "test_predictions = postprocess_data(sim_data_preproc, tower_locs)\n",
    "\n",
    "UTM_predictions = (test_predictions.groupby(['DateTime', 'TagID'], as_index=False)\n",
    "    .agg({'easting':'first',\n",
    "        'northing':'first',\n",
    "        'easting_pred':'mean',\n",
    "        'northing_pred':'mean'\n",
    "        })\n",
    ")\n",
    "\n",
    "# Calculate the Eucledian distance between the predicted and actual locations\n",
    "UTM_predictions['distance'] = np.sqrt((UTM_predictions['easting'] - UTM_predictions['easting_pred']) ** 2\n",
    "                    + (UTM_predictions['northing'] - UTM_predictions['northing_pred']) ** 2)\n",
    "\n",
    "# Calculate the mean absolute error of UTM_predictions['distance'] and the standard error\n",
    "mean_error = np.mean(UTM_predictions['distance'])\n",
    "std_error = stats.sem(UTM_predictions['distance'])\n",
    "\n",
    "print(f'Mean error (+/-SE) = {mean_error} (+/- {std_error})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAGzCAYAAADJ3dZzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA9yklEQVR4nO3de1hVZf7//9fmtEUQiJKNlAcSC1DU0hnFQ2lSRFY6YKWZolnOOGjjIT8NTpqZymSWjo6HaaavWuZUGllRmYiaWmiG2XhAw0mEkg2lAh5RYf3+6MeedmghIBuXz8d1rav2fd97rfeSilf3WvdaFsMwDAEAAJiUm6sLAAAAuJwIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIOwAAwNQIO8AVburUqbJYLPVyrF69eqlXr16Ozxs3bpTFYtGqVavq5fjDhg1Tq1at6uVYNXXixAk99thjCg4OlsVi0dixY11dEnDVI+wADcjSpUtlsVgcW6NGjRQSEqLY2FjNmzdPx48fr5PjHD58WFOnTtXOnTvrZH91qSHXVh0zZ87U0qVLNWrUKL322msaMmSIq0sCrnoW3o0FNBxLly7V8OHDNW3aNIWGhurcuXOy2+3auHGj0tPT1aJFC7333ntq37694zvnz5/X+fPn1ahRo2of54svvtBvfvMbLVmyRMOGDav2986ePStJ8vLykvTjzE7v3r21cuVKDRgwoNr7qWlt586dU0VFhaxWa50c63Lo2rWrPDw8tGXLFleXAuD/5+HqAgBUFRcXp86dOzs+Jycna/369br33nt1//33Kzs7W97e3pIkDw8PeXhc3n+VT506pcaNGztCjqt4enq69PjVUVRUpMjIyMt+nMqfyc+dP39eFRUVtfpZnTx5Uj4+PrUpD2hQuIwFXCHuuOMOTZ48WYcOHdLy5csd7Re6Zyc9PV09evRQQECAfH19dfPNN2vSpEmSfpyN+c1vfiNJGj58uOOS2dKlSyX9eF9Ou3btlJWVpdtuu02NGzd2fPfn9+xUKi8v16RJkxQcHCwfHx/df//9ys/PdxrTqlWrC84i/XSfv1bbhe7ZOXnypCZMmKDmzZvLarXq5ptv1uzZs/XzSWuLxaLRo0dr9erVateunaxWq9q2bas1a9Zc+A/8Z4qKijRixAjZbDY1atRIHTp00LJlyxz9lfcvHTx4UB988IGj9tzc3F/c7/Lly9WpUyd5e3srMDBQAwcOrPJnd7GfSW5uriwWi2bPnq25c+eqdevWslqt2rt3ryRp/fr16tmzp3x8fBQQEKB+/fopOzvbad+V//zs3btXDz/8sK655hr16NFDkmS32zV8+HDdcMMNslqtatasmfr16/er5wQ0NMzsAFeQIUOGaNKkSVq7dq0ef/zxC47Zs2eP7r33XrVv317Tpk2T1WrVgQMH9Omnn0qSIiIiNG3aNE2ZMkUjR45Uz549JUndunVz7OPIkSOKi4vTwIED9cgjj8hms/1iXTNmzJDFYtFTTz2loqIizZ07VzExMdq5c6djBqo6qlPbTxmGofvvv18bNmzQiBEj1LFjR3388ceaOHGivvvuO82ZM8dp/JYtW5Samqo//vGPatKkiebNm6eEhATl5eXp2muvvWhdp0+fVq9evXTgwAGNHj1aoaGhWrlypYYNG6bi4mL96U9/UkREhF577TWNGzdON9xwgyZMmCBJatq06S/+uU2ePFkPPvigHnvsMX3//feaP3++brvtNn355ZcKCAhwjP2ln8mSJUt05swZjRw5UlarVYGBgVq3bp3i4uJ04403aurUqTp9+rTmz5+v7t27a8eOHVVC4wMPPKA2bdpo5syZjqCYkJCgPXv2aMyYMWrVqpWKioqUnp6uvLy8Bn+jOODEANBgLFmyxJBkbN++/aJj/P39jVtuucXx+ZlnnjF++q/ynDlzDEnG999/f9F9bN++3ZBkLFmypErf7bffbkgyFi9efMG+22+/3fF5w4YNhiTj+uuvN0pLSx3tb731liHJ+Nvf/uZoa9mypZGYmPir+/yl2hITE42WLVs6Pq9evdqQZEyfPt1p3IABAwyLxWIcOHDA0SbJ8PLycmr76quvDEnG/Pnzqxzrp+bOnWtIMpYvX+5oO3v2rBEdHW34+vo6nXvLli2Nvn37/uL+DMMwcnNzDXd3d2PGjBlO7bt27TI8PDyc2i/2Mzl48KAhyfDz8zOKioqc+jp27GgEBQUZR44ccTpfNzc3Y+jQoY62yn9+Bg0a5PT9Y8eOGZKMF1544VfPBWjouIwFXGF8fX1/cVVW5WzAu+++q4qKihodw2q1avjw4dUeP3ToUDVp0sTxecCAAWrWrJk+/PDDGh2/uj788EO5u7vriSeecGqfMGGCDMPQRx995NQeExOj1q1bOz63b99efn5++uabb371OMHBwRo0aJCjzdPTU0888YROnDihTz755JJrT01NVUVFhR588EH98MMPji04OFht2rTRhg0bnMb/0s8kISHBaQapoKBAO3fu1LBhwxQYGOh0vnfeeecFfy5/+MMfnD57e3vLy8tLGzdu1LFjxy75/ICGhLADXGFOnDjhFCx+7qGHHlL37t312GOPyWazaeDAgXrrrbcuKfhcf/31l3SDa5s2bZw+WywWhYWFXfZ7Ow4dOqSQkJAqfx4RERGO/p9q0aJFlX1cc801v/rL/NChQ2rTpo3c3Jz/k3mx41RHTk6ODMNQmzZt1LRpU6ctOztbRUVFTuN/6WcSGhpapV5Juvnmm6uMjYiI0A8//KCTJ0/+4j6sVquef/55ffTRR7LZbLrttts0a9Ys2e32Sz5XwNW4Zwe4gnz77bcqKSlRWFjYRcd4e3tr06ZN2rBhgz744AOtWbNGb775pu644w6tXbtW7u7uv3qcS7nPprou9uDD8vLyatVUFy52HMMFT+CoqKiQxWLRRx99dMG6fH19nT7/0s+kLn5eF9rH2LFjdd9992n16tX6+OOPNXnyZKWkpGj9+vW65ZZban1MoL4wswNcQV577TVJUmxs7C+Oc3NzU58+ffTSSy9p7969mjFjhtavX++4NFLXT1zOyclx+mwYhg4cOOB0E+s111yj4uLiKt/9+azIpdTWsmVLHT58uMplvX379jn660LLli2Vk5NTZXasNsdp3bq1DMNQaGioYmJiqmxdu3atVb2StH///ip9+/bt03XXXVftpeWtW7fWhAkTtHbtWu3evVtnz57Viy++WOPaAFcg7ABXiPXr1+u5555TaGioBg8efNFxR48erdLWsWNHSVJZWZkkOX7RXSh81MSrr77qFDhWrVqlgoICxcXFOdpat26trVu3Oh5MKElpaWlVlllfSm333HOPysvL9fe//92pfc6cObJYLE7Hr4177rlHdrtdb775pqPt/Pnzmj9/vnx9fXX77bdf8j7j4+Pl7u6uZ599tsrMkmEYOnLkSI3rbdasmTp27Khly5Y5/Tnu3r1ba9eu1T333POr+zh16pTOnDnj1Na6dWs1adLE8c8RcKXgMhbQAH300Ufat2+fzp8/r8LCQq1fv17p6elq2bKl3nvvvV98WvK0adO0adMm9e3bVy1btlRRUZEWLlyoG264wfH8lNatWysgIECLFy9WkyZN5OPjoy5dulS5b6O6AgMD1aNHDw0fPlyFhYWaO3euwsLCnJbHP/bYY1q1apXuvvtuPfjgg/rvf/+r5cuXO90wfKm13Xffferdu7f+8pe/KDc3Vx06dNDatWv17rvvauzYsVX2XVMjR47UP/7xDw0bNkxZWVlq1aqVVq1apU8//VRz5879xXuoLqZ169aaPn26kpOTlZubq/79+6tJkyY6ePCg3nnnHY0cOVJPPvlkjWt+4YUXFBcXp+joaI0YMcKx9Nzf319Tp0791e9//fXX6tOnjx588EFFRkbKw8ND77zzjgoLCzVw4MAa1wW4hAtXggH4mcql55Wbl5eXERwcbNx5553G3/72N6clzpV+vvQ8IyPD6NevnxESEmJ4eXkZISEhxqBBg4yvv/7a6XvvvvuuERkZaXh4eDgt9b799tuNtm3bXrC+iy09//e//20kJycbQUFBhre3t9G3b1/j0KFDVb7/4osvGtdff71htVqN7t27G1988UWVff5SbT9fem4YhnH8+HFj3LhxRkhIiOHp6Wm0adPGeOGFF4yKigqncZKMpKSkKjVdbEn8zxUWFhrDhw83rrvuOsPLy8uIioq64PL46i49r/T2228bPXr0MHx8fAwfHx8jPDzcSEpKMvbv3+8Yc7GfSeXS84stD1+3bp3RvXt3w9vb2/Dz8zPuu+8+Y+/evU5jKv/5+fmjCn744QcjKSnJCA8PN3x8fAx/f3+jS5cuxltvvVXtcwMaCt6NBQAATI17dgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKnxUEH9+I6aw4cPq0mTJnX+GH0AAHB5GIah48ePKyQkpMqLen+KsCPp8OHDat68uavLAAAANZCfn68bbrjhov2EHcnxqPf8/Hz5+fm5uBoAAFAdpaWlat68+a++soWwo/+9ZdnPz4+wAwDAFebXbkHhBmUAAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqPFQQgGmVl5dr8+bNKigoULNmzdSzZ0+5u7u7uiwA9YyZHQCmlJqaqrCwMPXu3VsPP/ywevfurbCwMKWmprq6NAD1jLADwHRSU1M1YMAAFRYWOrUXFhZqwIABBB7gKkPYAWAq5eXlGjVqlAzDUJ8+fZSZmanjx48rMzNTffr0kWEYGjVqlMrLy11dKoB6QtgBYCobN25UUVGRevTooXfffVddu3aVr6+vunbtqnfffVfdu3dXUVGRNm7c6OpSAdQTwg4AU6kMMc8++6zc3Jz/E+fm5qapU6c6jQNgfoQdAABgaoQdAKbSq1cvSdIzzzyjiooKp76Kigo9++yzTuMAmB9hB4Cp9OrVS02bNtWWLVvUr18/pxuU+/Xrpy1btigoKIiwA1xFXBp2ysvLNXnyZIWGhsrb21utW7fWc889J8MwHGMMw9CUKVPUrFkzeXt7KyYmRjk5OU77OXr0qAYPHiw/Pz8FBARoxIgROnHiRH2fDoAGwN3dXYsXL5YkZWRkqFu3bvLz81O3bt20fv16SdKiRYt4uCBwFXFp2Hn++ee1aNEi/f3vf1d2draef/55zZo1S/Pnz3eMmTVrlubNm6fFixdr27Zt8vHxUWxsrM6cOeMYM3jwYO3Zs0fp6elKS0vTpk2bNHLkSFecEoAGID4+Xm+//baCgoKc2oOCgvT2228rPj7eRZUBcAWL8dNplHp27733ymaz6ZVXXnG0JSQkyNvbW8uXL5dhGAoJCdGECRP05JNPSpJKSkpks9m0dOlSDRw4UNnZ2YqMjNT27dvVuXNnSdKaNWt0zz336Ntvv1VISMiv1lFaWip/f3+VlJTIz8/v8pwsgHrH6yIAc6vu72+Xzux069ZNGRkZ+vrrryVJX331lbZs2aK4uDhJ0sGDB2W32xUTE+P4jr+/v7p06aLMzExJUmZmpgICAhxBR5JiYmLk5uambdu2XfC4ZWVlKi0tddoAmI+7u7t69eqlQYMGqVevXgQd4Crl0heB/vnPf1ZpaanCw8Pl7u6u8vJyzZgxQ4MHD5Yk2e12SZLNZnP6ns1mc/TZ7fYqU9UeHh4KDAx0jPm5lJQUx4oMAABgbi6d2Xnrrbf0+uuva8WKFdqxY4eWLVum2bNna9myZZf1uMnJySopKXFs+fn5l/V4AADAdVw6szNx4kT9+c9/1sCBAyVJUVFROnTokFJSUpSYmKjg4GBJP768r1mzZo7vFRYWqmPHjpKk4OBgFRUVOe33/PnzOnr0qOP7P2e1WmW1Wi/DGQEAgIbGpTM7p06dqvI4d3d3d8eDwEJDQxUcHKyMjAxHf2lpqbZt26bo6GhJUnR0tIqLi5WVleUYs379elVUVKhLly71cBYAAKAhc+nMzn333acZM2aoRYsWatu2rb788ku99NJLevTRRyVJFotFY8eO1fTp09WmTRuFhoZq8uTJCgkJUf/+/SVJERERuvvuu/X4449r8eLFOnfunEaPHq2BAwdWayUWAAAwN5eGnfnz52vy5Mn64x//qKKiIoWEhOj3v/+9pkyZ4hjzf//3fzp58qRGjhyp4uJi9ejRQ2vWrFGjRo0cY15//XWNHj1affr0kZubmxISEjRv3jxXnBIAAGhgXPqcnYaC5+wAAHDluSKeswMAAHC5EXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpEXYAAICpebi6AAC4XMrLy7V582YVFBSoWbNm6tmzp9zd3V1dFoB6xswOAFNKTU1VWFiYevfurYcffli9e/dWWFiYUlNTXV0agHpG2AFgOqmpqRowYICioqKUmZmp48ePKzMzU1FRURowYACBB7jKWAzDMFxdhKuVlpbK399fJSUl8vPzc3U5AGqhvLxcYWFhioqK0urVq+Xm9r//p6uoqFD//v21e/du5eTkcEkLuMJV9/c3MzsATGXz5s3Kzc3VpEmTnIKOJLm5uSk5OVkHDx7U5s2bXVQhgPpG2AFgKgUFBZKkdu3aXbC/sr1yHADzI+wAMJVmzZpJknbv3n3B/sr2ynEAzI+wA8BUevbsqVatWmnmzJmqqKhw6quoqFBKSopCQ0PVs2dPF1UIoL4RdgCYiru7u1588UWlpaWpf//+Tqux+vfvr7S0NM2ePZubk4GrCA8VBGA68fHxWrVqlSZMmKBu3bo52kNDQ7Vq1SrFx8e7sDoA9Y2l52LpOWBWPEEZMLcrYul5q1atZLFYqmxJSUmSpDNnzigpKUnXXnutfH19lZCQoMLCQqd95OXlqW/fvmrcuLGCgoI0ceJEnT9/3hWnA6CBcXd3V69evTRo0CD16tWLoANcpVwadrZv366CggLHlp6eLkl64IEHJEnjxo3T+++/r5UrV+qTTz7R4cOHnaafy8vL1bdvX509e1afffaZli1bpqVLl2rKlCkuOR8AANDwNKjLWGPHjlVaWppycnJUWlqqpk2basWKFRowYIAkad++fYqIiFBmZqa6du2qjz76SPfee68OHz4sm80mSVq8eLGeeuopff/99/Ly8rrgccrKylRWVub4XFpaqubNm3MZCwCAK8gVcRnrp86ePavly5fr0UcflcViUVZWls6dO6eYmBjHmPDwcLVo0UKZmZmS5HjXTWXQkaTY2FiVlpZqz549Fz1WSkqK/P39HVvz5s0v34kBAACXajBhZ/Xq1SouLtawYcMkSXa7XV5eXgoICHAaZ7PZZLfbHWN+GnQq+yv7LiY5OVklJSWOLT8/v+5OBAAANCgNZun5K6+8ori4OIWEhFz2Y1mtVlmt1st+HAAA4HoNYmbn0KFDWrdunR577DFHW3BwsM6ePavi4mKnsYWFhQoODnaM+fnqrMrPlWMAAMDVrUGEnSVLligoKEh9+/Z1tHXq1Emenp7KyMhwtO3fv195eXmKjo6WJEVHR2vXrl0qKipyjElPT5efn58iIyPr7wQAAECD5fLLWBUVFVqyZIkSExPl4fG/cvz9/TVixAiNHz9egYGB8vPz05gxYxQdHa2uXbtKku666y5FRkZqyJAhmjVrlux2u55++mklJSVxmQoAAEhqAGFn3bp1ysvL06OPPlqlb86cOXJzc1NCQoLKysoUGxurhQsXOvrd3d2VlpamUaNGKTo6Wj4+PkpMTNS0adPq8xQAAEAD1qCes+MqvC4CMCdeFwGY2xX3nB0AqEupqakKCwtT79699fDDD6t3794KCwtTamqqq0sDUM8IOwBMJzU1VQMGDFBUVJQyMzN1/Phxx0NIBwwYQOABrjJcxhKXsQAzKS8vV1hYmKKiovT222/r008/dVzG6t69uxISErR7927l5ORwSQu4wnEZC8BVafPmzcrNzVW3bt100003OV3GuummmxQdHa2DBw9q8+bNri4VQD1x+WosAKhLBQUFkqRJkyapUaNGTn2FhYX6y1/+4jQOgPkxswPAVIKCgiRJhmGoT58+Tvfs9OnTR5VX7ivHATA/ZnYAmEpFRYUk6ZprrtE777zjeFhp165d9c477ygoKEjHjh1zjANgfszsADCVTZs2SZKOHTum+Ph4p5md+Ph4HTt2zGkcAPMj7AAwpalTp2rXrl3q1q2b/Pz81K1bN+3evVvPPPOMq0sDUM8IOwBMpVevXpJ+fBXN119/rQ0bNmjFihXasGGD9u/f73i5cOU4AOZH2AFgKr169VLTpk21ZcsWxcfHy2q16t5775XValV8fLy2bNmioKAgwg5wFeEGZQCm4u7ursWLFyshIUEZGRlKS0tz9DVu3FiStGjRIh4oCFxFmNkBYDrx8fF6++23qywvDwoK0ttvv634+HgXVQbAFXhdhHhdBGBWvPUcMLfq/v7mMhYA03J3d+feHABcxgIAAOZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKbGaiwApsXScwASMzsATCo1NVVhYWHq3bu3Hn74YfXu3VthYWFKTU11dWkA6hlhB4DppKamasCAAYqKilJmZqaOHz+uzMxMRUVFacCAAQQe4CrDE5TFE5QBMykvL1dYWJiioqK0evVqubn97//pKioq1L9/f+3evVs5OTlc0gKucNX9/c3MDgBT2bx5s3JzczVp0iSnoCNJbm5uSk5O1sGDB7V582YXVQigvhF2AJhKQUGBJKldu3YX7K9srxwHwPwIOwBMpVmzZpKk3bt3X7C/sr1yHADzI+wAMJWePXuqVatWmjlzpioqKpz6KioqlJKSotDQUPXs2dNFFQKob4QdAKbi7u6uF198UWlpaerfv7/Taqz+/fsrLS1Ns2fP5uZk4CrCQwUBmE58fLxWrVqlCRMmqFu3bo720NBQrVq1SvHx8S6sDkB9c/nMznfffadHHnlE1157rby9vRUVFaUvvvjC0W8YhqZMmaJmzZrJ29tbMTExysnJcdrH0aNHNXjwYPn5+SkgIEAjRozQiRMn6vtUADQg8fHxOnDggDZs2KAVK1Zow4YNysnJIegAVyGXzuwcO3ZM3bt3V+/evfXRRx+padOmysnJ0TXXXOMYM2vWLM2bN0/Lli1TaGioJk+erNjYWO3du1eNGjWSJA0ePFgFBQVKT0/XuXPnNHz4cI0cOVIrVqxw1akBaADc3d3Vq1cvV5cBwMVc+lDBP//5z/r0008v+rwLwzAUEhKiCRMm6Mknn5QklZSUyGazaenSpRo4cKCys7MVGRmp7du3q3PnzpKkNWvW6J577tG3336rkJCQX62DhwoCAHDluSIeKvjee++pc+fOeuCBBxQUFKRbbrlF//znPx39Bw8elN1uV0xMjKPN399fXbp0UWZmpiQpMzNTAQEBjqAjSTExMXJzc9O2bdsueNyysjKVlpY6bQAAwJxcGna++eYbLVq0SG3atNHHH3+sUaNG6YknntCyZcskSXa7XZJks9mcvmez2Rx9drtdQUFBTv0eHh4KDAx0jPm5lJQU+fv7O7bmzZvX9akBAIAGwqVhp6KiQrfeeqtmzpypW265RSNHjtTjjz+uxYsXX9bjJicnq6SkxLHl5+df1uMBAADXcWnYadasmSIjI53aIiIilJeXJ0kKDg6WJBUWFjqNKSwsdPQFBwerqKjIqf/8+fM6evSoY8zPWa1W+fn5OW0AAMCcXBp2unfvrv379zu1ff3112rZsqWkH5+JERwcrIyMDEd/aWmptm3bpujoaElSdHS0iouLlZWV5Rizfv16VVRUqEuXLvVwFgAAoCFz6dLzcePGqVu3bpo5c6YefPBBff7553r55Zf18ssvS5IsFovGjh2r6dOnq02bNo6l5yEhIerfv7+kH2eC7r77bsflr3Pnzmn06NEaOHBgtVZiAQAAc3Pp0nNJSktLU3JysnJychQaGqrx48fr8ccfd/QbhqFnnnlGL7/8soqLi9WjRw8tXLhQN910k2PM0aNHNXr0aL3//vtyc3NTQkKC5s2bJ19f32rVwNJzAACuPNX9/e3ysNMQEHYAALjyVPf3N+/GAtDgnDp1Svv27auTfZ0+fVq5ublq1aqVvL29a72/8PBwNW7cuA4qA1BfCDsAGpx9+/apU6dOri7jgrKysnTrrbe6ugwAl4CwA6DBCQ8Pd1phWRvZ2dl65JFHtHz5ckVERNR6f+Hh4XVQFYD6RNgB0OA0bty4zmdPIiIimJEBrlIufc4OAADA5UbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApuZR0y/m5eXp0KFDOnXqlJo2baq2bdvKarXWZW0AAAC1dklhJzc3V4sWLdIbb7yhb7/9VoZhOPq8vLzUs2dPjRw5UgkJCXJzY9IIAAC4XrUTyRNPPKEOHTro4MGDmj59uvbu3auSkhKdPXtWdrtdH374oXr06KEpU6aoffv22r59++WsGwAAoFqqHXZ8fHz0zTff6K233tKQIUN08803q0mTJvLw8FBQUJDuuOMOPfPMM8rOztbs2bOVn5//q/ucOnWqLBaL0xYeHu7oP3PmjJKSknTttdfK19dXCQkJKiwsdNpHXl6e+vbtq8aNGysoKEgTJ07U+fPnL+GPAAAAmFm1L2OlpKRUe6d33313tce2bdtW69at+19BHv8rady4cfrggw+0cuVK+fv7a/To0YqPj9enn34qSSovL1ffvn0VHByszz77TAUFBRo6dKg8PT01c+bMatcAAADMq8Y3KNdZAR4eCg4OrtJeUlKiV155RStWrNAdd9whSVqyZIkiIiK0detWde3aVWvXrtXevXu1bt062Ww2dezYUc8995yeeuopTZ06VV5eXvV9OgAAoIGp0V3ER44cUVJSkiIjI3XdddcpMDDQabsUOTk5CgkJ0Y033qjBgwcrLy9PkpSVlaVz584pJibGMTY8PFwtWrRQZmamJCkzM1NRUVGy2WyOMbGxsSotLdWePXsuesyysjKVlpY6bQAAwJxqNLMzZMgQHThwQCNGjJDNZpPFYqnRwbt06aKlS5fq5ptvVkFBgZ599ln17NlTu3fvlt1ul5eXlwICApy+Y7PZZLfbJUl2u90p6FT2V/ZdTEpKip599tka1QwAAK4sNQo7mzdv1pYtW9ShQ4daHTwuLs7x9+3bt1eXLl3UsmVLvfXWW/L29q7Vvn9JcnKyxo8f7/hcWlqq5s2bX7bjAQAA16nRZazw8HCdPn26rmtRQECAbrrpJh04cEDBwcE6e/asiouLncYUFhY67vEJDg6usjqr8vOF7gOqZLVa5efn57QBAABzqlHYWbhwof7yl7/ok08+0ZEjR+rs/pcTJ07ov//9r5o1a6ZOnTrJ09NTGRkZjv79+/crLy9P0dHRkqTo6Gjt2rVLRUVFjjHp6eny8/NTZGRkjesAAADmUaPLWAEBASotLXWskqpkGIYsFovKy8urtZ8nn3xS9913n1q2bKnDhw/rmWeekbu7uwYNGiR/f3+NGDFC48ePV2BgoPz8/DRmzBhFR0era9eukqS77rpLkZGRGjJkiGbNmiW73a6nn35aSUlJvLoCAABIqmHYGTx4sDw9PbVixYpa3aD87bffatCgQTpy5IiaNm2qHj16aOvWrWratKkkac6cOXJzc1NCQoLKysoUGxurhQsXOr7v7u6utLQ0jRo1StHR0fLx8VFiYqKmTZtWo3oAAID5WIyfvuCqmho3bqwvv/xSN9988+Woqd6VlpbK399fJSUl3L8DmMyOHTvUqVMnZWVl6dZbb3V1OQDqUHV/f9fonp3OnTtX63UQAAAArlajy1hjxozRn/70J02cOFFRUVHy9PR06m/fvn2dFAcAAFBbNQo7Dz30kCTp0UcfdbRZLJZLvkEZAADgcqtR2Dl48GBd1wEAAHBZ1CjstGzZsq7rAAAAuCyqfYPy1q1bq73TU6dO/eKLOAEAAOpLtcPOkCFDFBsbq5UrV+rkyZMXHLN3715NmjRJrVu3VlZWVp0VCQAAUFPVvoy1d+9eLVq0SE8//bQefvhh3XTTTQoJCVGjRo107Ngx7du3TydOnNDvfvc7rV27VlFRUZezbgAAgGqpdtjx9PTUE088oSeeeEJffPGFtmzZokOHDun06dPq0KGDxo0bp969eyswMPBy1gsAAHBJanSDcufOndW5c+e6rgUAAKDO1egJygAAAFcKwg4AADA1wg4AADA1wg4AADC1WoedM2fO1EUdAAAAl0WNwk5FRYWee+45XX/99fL19dU333wjSZo8ebJeeeWVOi0QAACgNmoUdqZPn66lS5dq1qxZ8vLycrS3a9dO//rXv+qsOAAAgNqqUdh59dVX9fLLL2vw4MFyd3d3tHfo0EH79u2rs+IAAABqq0Zh57vvvlNYWFiV9oqKCp07d67WRQEAANSVGoWdyMhIbd68uUr7qlWrdMstt9S6KAAAgLpSo9dFTJkyRYmJifruu+9UUVGh1NRU7d+/X6+++qrS0tLqukYAAIAaq9HMTr9+/fT+++9r3bp18vHx0ZQpU5Sdna33339fd955Z13XCAAAUGM1mtmRpJ49eyo9Pb0uawEAAKhzNZrZ2b59u7Zt21alfdu2bfriiy9qXRQAAEBdqVHYSUpKUn5+fpX27777TklJSbUuCgAAoK7UKOzs3btXt956a5X2W265RXv37q11UQAAAHWlRmHHarWqsLCwSntBQYE8PGp8GxAAAECdq1HYueuuu5ScnKySkhJHW3FxsSZNmsRqLAAA0KDUaBpm9uzZuu2229SyZUvHQwR37twpm82m1157rU4LBAAAqI0ahZ3rr79e//nPf/T666/rq6++kre3t4YPH65BgwbJ09OzrmsEAACosRpdxpIkHx8fjRw5UgsWLNDs2bM1dOjQWgWdv/71r7JYLBo7dqyj7cyZM0pKStK1114rX19fJSQkVLlXKC8vT3379lXjxo0VFBSkiRMn6vz58zWuAwAAmEuN7ybOycnRhg0bVFRUpIqKCqe+KVOmXNK+tm/frn/84x9q3769U/u4ceP0wQcfaOXKlfL399fo0aMVHx+vTz/9VJJUXl6uvn37Kjg4WJ999pkKCgocoWvmzJk1PTUAAGAiNQo7//znPzVq1Chdd911Cg4OlsVicfRZLJZLCjsnTpzQ4MGD9c9//lPTp093tJeUlOiVV17RihUrdMcdd0iSlixZooiICG3dulVdu3bV2rVrtXfvXq1bt042m00dO3bUc889p6eeekpTp06Vl5dXTU4PAACYSI0uY02fPl0zZsyQ3W7Xzp079eWXXzq2HTt2XNK+kpKS1LdvX8XExDi1Z2Vl6dy5c07t4eHhatGihTIzMyVJmZmZioqKks1mc4yJjY1VaWmp9uzZc9FjlpWVqbS01GkDAADmVKOZnWPHjumBBx6o9cHfeOMN7dixQ9u3b6/SZ7fb5eXlpYCAAKd2m80mu93uGPPToFPZX9l3MSkpKXr22WdrWT0AALgS1Ghm54EHHtDatWtrdeD8/Hz96U9/0uuvv65GjRrVal+XqvIZQZXbhV59AQAAzKFGMzthYWGaPHmytm7dqqioqCqrsJ544olf3UdWVpaKioqcXjtRXl6uTZs26e9//7s+/vhjnT17VsXFxU6zO4WFhQoODpYkBQcH6/PPP3fab+VqrcoxF2K1WmW1Wn+1RgAAcOWrUdh5+eWX5evrq08++USffPKJU5/FYqlW2OnTp4927drl1DZ8+HCFh4frqaeeUvPmzeXp6amMjAwlJCRIkvbv36+8vDxFR0dLkqKjozVjxgwVFRUpKChIkpSeni4/Pz9FRkbW5NQAAIDJ1CjsHDx4sNYHbtKkidq1a+fU5uPjo2uvvdbRPmLECI0fP16BgYHy8/PTmDFjFB0dra5du0r68bUVkZGRGjJkiGbNmiW73a6nn35aSUlJzNwAAABJtXjOTn2YM2eO3NzclJCQoLKyMsXGxmrhwoWOfnd3d6WlpWnUqFGKjo6Wj4+PEhMTNW3aNBdWDQAAGhKLYRhGTb747bff6r333lNeXp7Onj3r1PfSSy/VSXH1pbS0VP7+/iopKZGfn5+rywFQh3bs2KFOnTopKyvL6R5BAFe+6v7+rtHMTkZGhu6//37deOON2rdvn9q1a6fc3FwZhsF/TAAAQINSo6XnycnJevLJJ7Vr1y41atRIb7/9tvLz83X77bfXyfN3AAAA6kqNwk52draGDh0qSfLw8NDp06fl6+uradOm6fnnn6/TAgEAAGqjRmHHx8fHcZ9Os2bN9N///tfR98MPP9RNZQAAAHWgRvfsdO3aVVu2bFFERITuueceTZgwQbt27VJqaqpjWTgAAEBDUKOw89JLL+nEiROSpGeffVYnTpzQm2++qTZt2lxxK7EAAIC51Sjs3HjjjY6/9/Hx0eLFi+usIAAAgLpUo3t2brzxRh05cqRKe3FxsVMQAgAAcLUahZ3c3FyVl5dXaS8rK9N3331X66IAAADqyiVdxnrvvfccf//xxx/L39/f8bm8vFwZGRlq1apVnRUHAABQW5cUdvr37y/pxzebJyYmOvV5enqqVatWevHFF+usOAAAgNq6pLBTUVEhSQoNDdX27dt13XXXXZaiAAAA6kqNVmMdPHiwSltxcbECAgJqWw8AAECdqtENys8//7zefPNNx+cHHnhAgYGBuv766/XVV1/VWXEAAAC1VaOws3jxYjVv3lySlJ6ernXr1mnNmjWKi4vTxIkT67RAAACA2qjRZSy73e4IO2lpaXrwwQd11113qVWrVurSpUudFggAAFAbNZrZueaaa5Sfny9JWrNmjWJiYiRJhmFc8Pk7AAAArlKjmZ34+Hg9/PDDatOmjY4cOaK4uDhJ0pdffqmwsLA6LRAAAKA2ahR25syZo1atWik/P1+zZs2Sr6+vJKmgoEB//OMf67RAAACA2qhR2PH09NSTTz5ZpX3cuHG1LggAAKAuVTvsvPfee4qLi5Onp6fTayMu5P777691YQAAAHWh2mGnf//+stvtCgoKcrw24kIsFgs3KQMAgAaj2mGn8lURP/97AACAhqxGS88BAACuFJd8g3JFRYWWLl2q1NRU5ebmymKxKDQ0VAMGDNCQIUNksVguR50AAAA1ckkzO4Zh6P7779djjz2m7777TlFRUWrbtq0OHTqkYcOG6Xe/+93lqhMAAKBGLmlmZ+nSpdq0aZMyMjLUu3dvp77169erf//+evXVVzV06NA6LRIAAKCmLmlm59///rcmTZpUJehI0h133KE///nPev311+usOAAAgNq6pLDzn//8R3ffffdF++Pi4vTVV1/VuigAAIC6ckmXsY4ePSqbzXbRfpvNpmPHjtW6KABXppycHB0/ftzVZTjJzs52+mtD0aRJE7Vp08bVZQBXhUsKO+Xl5fLwuPhX3N3ddf78+VoXBeDKk5OTo5tuusnVZVzUI4884uoSqvj6668JPEA9uKSwYxiGhg0bJqvVesH+srKySzr4okWLtGjRIuXm5kqS2rZtqylTpjjeon7mzBlNmDBBb7zxhsrKyhQbG6uFCxc6zS7l5eVp1KhR2rBhg3x9fZWYmKiUlJRfDGUA6l7ljM7y5csVERHh4mr+5/Tp08rNzVWrVq3k7e3t6nIk/TjL9MgjjzS4WTDArC4pESQmJv7qmEtZiXXDDTfor3/9q9q0aSPDMLRs2TL169dPX375pdq2batx48bpgw8+0MqVK+Xv76/Ro0crPj5en376qaQfZ5r69u2r4OBgffbZZyooKNDQoUPl6empmTNnXsqpAagjERERuvXWW11dhpPu3bu7ugQArmQ0MNdcc43xr3/9yyguLjY8PT2NlStXOvqys7MNSUZmZqZhGIbx4YcfGm5ubobdbneMWbRokeHn52eUlZVV+5glJSWGJKOkpKTuTgS4ymRlZRmSjKysLFeX0uDxZwXUjer+/m4wr4soLy/XG2+8oZMnTyo6OlpZWVk6d+6cYmJiHGPCw8PVokULZWZmSpIyMzMVFRXldFkrNjZWpaWl2rNnz0WPVVZWptLSUqcNAACYk8vDzq5du+Tr6yur1ao//OEPeueddxQZGSm73S4vLy8FBAQ4jbfZbLLb7ZIku91eZXVY5efKMReSkpIif39/x9a8efO6PSkAANBguDzs3Hzzzdq5c6e2bdumUaNGKTExUXv37r2sx0xOTlZJSYljy8/Pv6zHAwAAruPyJUteXl4KCwuTJHXq1Enbt2/X3/72Nz300EM6e/asiouLnWZ3CgsLFRwcLEkKDg7W559/7rS/wsJCR9/FWK3Wi64oAwAA5uLymZ2fq6ioUFlZmTp16iRPT09lZGQ4+vbv36+8vDxFR0dLkqKjo7Vr1y4VFRU5xqSnp8vPz0+RkZH1XjsAAGh4XDqzk5ycrLi4OLVo0ULHjx/XihUrtHHjRn388cfy9/fXiBEjNH78eAUGBsrPz09jxoxRdHS0unbtKkm66667FBkZqSFDhmjWrFmy2+16+umnlZSUxMwNAACQ5OKwU1RUpKFDh6qgoED+/v5q3769Pv74Y915552SpDlz5sjNzU0JCQlODxWs5O7urrS0NI0aNUrR0dHy8fFRYmKipk2b5qpTAgAADYxLw84rr7zyi/2NGjXSggULtGDBgouOadmypT788MO6Lg0AAJhEg7tnBwAAoC4RdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKkRdgAAgKm5NOykpKToN7/5jZo0aaKgoCD1799f+/fvdxpz5swZJSUl6dprr5Wvr68SEhJUWFjoNCYvL099+/ZV48aNFRQUpIkTJ+r8+fP1eSoAAKCBcmnY+eSTT5SUlKStW7cqPT1d586d01133aWTJ086xowbN07vv/++Vq5cqU8++USHDx9WfHy8o7+8vFx9+/bV2bNn9dlnn2nZsmVaunSppkyZ4opTAgAADYyHKw++Zs0ap89Lly5VUFCQsrKydNttt6mkpESvvPKKVqxYoTvuuEOStGTJEkVERGjr1q3q2rWr1q5dq71792rdunWy2Wzq2LGjnnvuOT311FOaOnWqvLy8qhy3rKxMZWVljs+lpaWX90QBAIDLNKh7dkpKSiRJgYGBkqSsrCydO3dOMTExjjHh4eFq0aKFMjMzJUmZmZmKioqSzWZzjImNjVVpaan27NlzweOkpKTI39/fsTVv3vxynRIAAHAxl87s/FRFRYXGjh2r7t27q127dpIku90uLy8vBQQEOI212Wyy2+2OMT8NOpX9lX0XkpycrPHjxzs+l5aWEniAWrKcP6Nbgt3kXfy1dLhB/X9Ug+Nd/LVuCXaT5fwZV5cCXBUaTNhJSkrS7t27tWXLlst+LKvVKqvVetmPA1xNGp3I047f+0qbfi9tcnU1DVuEpB2/91X2iTxJ3VxdDmB6DSLsjB49Wmlpadq0aZNuuOEGR3twcLDOnj2r4uJip9mdwsJCBQcHO8Z8/vnnTvurXK1VOQbA5XfGt4Vu/ccJvf7664oID3d1OQ1a9r59Gjx4sF65p4WrSwGuCi4NO4ZhaMyYMXrnnXe0ceNGhYaGOvV36tRJnp6eysjIUEJCgiRp//79ysvLU3R0tCQpOjpaM2bMUFFRkYKCgiRJ6enp8vPzU2RkZP2eEHAVMzwa6Ut7hU4H3CSFdHR1OQ3aaXuFvrRXyPBo5OpSgKuCS8NOUlKSVqxYoXfffVdNmjRx3GPj7+8vb29v+fv7a8SIERo/frwCAwPl5+enMWPGKDo6Wl27dpUk3XXXXYqMjNSQIUM0a9Ys2e12Pf3000pKSuJSFQAAcG3YWbRokSSpV69eTu1LlizRsGHDJElz5syRm5ubEhISVFZWptjYWC1cuNAx1t3dXWlpaRo1apSio6Pl4+OjxMRETZs2rb5OAwAANGAuv4z1axo1aqQFCxZowYIFFx3TsmVLffjhh3VZGgAAMAnWhwIAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFMj7AAAAFPzcHUBAMzh1KlTkqQdO3a4uBJnp0+fVm5urlq1aiVvb29XlyNJys7OdnUJwFWFsAOgTuzbt0+S9Pjjj7u4kitHkyZNXF0CcFUg7ACoE/3795ckhYeHq3Hjxq4t5ieys7P1yCOPaPny5YqIiHB1OQ5NmjRRmzZtXF0GcFUg7ACoE9ddd50ee+wxV5dxUREREbr11ltdXQYAF3DpDcqbNm3Sfffdp5CQEFksFq1evdqp3zAMTZkyRc2aNZO3t7diYmKUk5PjNObo0aMaPHiw/Pz8FBAQoBEjRujEiRP1eBYAAKAhc2nYOXnypDp06KAFCxZcsH/WrFmaN2+eFi9erG3btsnHx0exsbE6c+aMY8zgwYO1Z88epaenKy0tTZs2bdLIkSPr6xQAAEAD59LLWHFxcYqLi7tgn2EYmjt3rp5++mn169dPkvTqq6/KZrNp9erVGjhwoLKzs7VmzRpt375dnTt3liTNnz9f99xzj2bPnq2QkJB6OxcAANAwNdjn7Bw8eFB2u10xMTGONn9/f3Xp0kWZmZmSpMzMTAUEBDiCjiTFxMTIzc1N27Ztu+i+y8rKVFpa6rQBAABzarBhx263S5JsNptTu81mc/TZ7XYFBQU59Xt4eCgwMNAx5kJSUlLk7+/v2Jo3b17H1QMAgIaiwYadyyk5OVklJSWOLT8/39UlAQCAy6TBhp3g4GBJUmFhoVN7YWGhoy84OFhFRUVO/efPn9fRo0cdYy7EarXKz8/PaQMAAObUYMNOaGiogoODlZGR4WgrLS3Vtm3bFB0dLUmKjo5WcXGxsrKyHGPWr1+viooKdenSpd5rBgAADY9LV2OdOHFCBw4ccHw+ePCgdu7cqcDAQLVo0UJjx47V9OnT1aZNG4WGhmry5MkKCQlxPKk1IiJCd999tx5//HEtXrxY586d0+jRozVw4EBWYgEAAEkuDjtffPGFevfu7fg8fvx4SVJiYqKWLl2q//u//9PJkyc1cuRIFRcXq0ePHlqzZo0aNWrk+M7rr7+u0aNHq0+fPnJzc1NCQoLmzZtX7+cCAAAaJothGIari3C10tJS+fv7q6SkhPt3AJPZsWOHOnXqpKysLF4XAZhMdX9/N9h7dgAAAOoCYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJgaYQcAAJiah6sLAICfO3XqlPbt21cn+8rOznb6a22Fh4ercePGdbIvAPWDsAOgwdm3b586depUp/t85JFH6mQ/WVlZuvXWW+tkXwDqh2nCzoIFC/TCCy/IbrerQ4cOmj9/vn7729+6uiwANRAeHq6srKw62dfp06eVm5urVq1aydvbu9b7Cw8Pr4OqANQni2EYhquLqK0333xTQ4cO1eLFi9WlSxfNnTtXK1eu1P79+xUUFPSr3y8tLZW/v79KSkrk5+dXDxUDAIDaqu7vb1PcoPzSSy/p8ccf1/DhwxUZGanFixercePG+n//7/+5ujQAAOBiV3zYOXv2rLKyshQTE+Noc3NzU0xMjDIzMy/4nbKyMpWWljptAADAnK74sPPDDz+ovLxcNpvNqd1ms8lut1/wOykpKfL393dszZs3r49SAQCAC1zxYacmkpOTVVJS4tjy8/NdXRIAALhMrvjVWNddd53c3d1VWFjo1F5YWKjg4OALfsdqtcpqtdZHeQAAwMWu+JkdLy8vderUSRkZGY62iooKZWRkKDo62oWVAQCAhuCKn9mRpPHjxysxMVGdO3fWb3/7W82dO1cnT57U8OHDXV0aAABwMVOEnYceekjff/+9pkyZIrvdro4dO2rNmjVVbloGAABXH1M8VLC2eKggAABXnqvqoYIAAAAXQ9gBAACmRtgBAACmRtgBAACmZorVWLVVeY8278gCAODKUfl7+9fWWhF2JB0/flySeEcWAABXoOPHj8vf3/+i/Sw9149PXD58+LCaNGkii8Xi6nIA1KHS0lI1b95c+fn5PFoCMBnDMHT8+HGFhITIze3id+YQdgCYGs/RAsANygAAwNQIOwAAwNQIOwBMzWq16plnnpHVanV1KQBchHt2AACAqTGzAwAATI2wAwAATI2wAwAATI2wAwAATI2wAwAATI2wA8CUNm3apPvuu08hISGyWCxavXq1q0sC4CKEHQCmdPLkSXXo0EELFixwdSkAXIy3ngMwpbi4OMXFxbm6DAANADM7AADA1Ag7AADA1Ag7AADA1Ag7AADA1Ag7AADA1FiNBcCUTpw4oQMHDjg+Hzx4UDt37lRgYKBatGjhwsoA1DeLYRiGq4sAgLq2ceNG9e7du0p7YmKili5dWv8FAXAZwg4AADA17tkBAACmRtgBAACmRtgBAACmRtgBAACmRtgBAACmRtgBAACmRtgBAACmRtgBAACmRtgBAACmRtgBAACmRtgBAACm9v8BFnbsyzhgJowAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the distribution of the errors\n",
    "def plot_error_distribution(distances):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.boxplot(distances)\n",
    "    ax.set_ylabel('Distance (m)')\n",
    "    ax.set_title('Distribution of errors')\n",
    "    plt.show()\n",
    "\n",
    "plot_error_distribution(UTM_predictions['distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[119], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Working code for smoothing the predictions, this should be incorporated into the above code as a variation to calculate the locations\n",
    "#potentially add a spline?, or use a spline in the place of the average? Lowess smoothing function looks good\n",
    "\n",
    "######## update this code to apply for both the x and y offset\n",
    "max_gap_4_smoothing = 10 # in minutes\n",
    "\n",
    "UTM_predictions = UTM_predictions.sort_values(by='DateTime')\n",
    "tag_ids = UTM_predictions['TagID'].unique()\n",
    "\n",
    "# Loop through each unique value of 'TagID'\n",
    "for tag_id in tag_ids:\n",
    "    # Filter the dataframe to only include rows with the current 'TagID'\n",
    "    UTM_predictions_filtered = UTM_predictions[UTM_predictions['TagID'] == tag_id].copy()\n",
    "    \n",
    "    # Calculate the difference in minutes between each DateTime\n",
    "    UTM_predictions_filtered['diff'] = UTM_predictions_filtered['DateTime'].diff().dt.total_seconds() / 60\n",
    "    \n",
    "    # Find the indices where the difference is greater than 10 minutes\n",
    "    split_indices = UTM_predictions_filtered[UTM_predictions_filtered['diff'] > max_gap_4_smoothing].index\n",
    "    \n",
    "    # Split the filtered dataframe into subsets based on the split_indices\n",
    "    split_data = np.split(UTM_predictions_filtered, split_indices)\n",
    "    \n",
    "    # Loop through each subset of the dataframe\n",
    "    for subset in split_data:\n",
    "        index_list = subset.index\n",
    "        if len(subset) < 2:\n",
    "            UTM_predictions.loc[index_list, 'easting_pred_smooth'] = UTM_predictions.loc[index_list, 'easting_pred']\n",
    "            UTM_predictions.loc[index_list, 'northing_pred_smooth'] = UTM_predictions.loc[index_list, 'northing_pred']\n",
    "        else:\n",
    "            smoothed_values_easting = lowess(subset['easting_pred'], subset['DateTime'], is_sorted=False, return_sorted=False)\n",
    "            smoothed_values_northing = lowess(subset['northing_pred'], subset['DateTime'], is_sorted=False, return_sorted=False)\n",
    "            UTM_predictions.loc[index_list, 'easting_pred_smooth'] = smoothed_values_easting\n",
    "            UTM_predictions.loc[index_list, 'northing_pred_smooth'] = smoothed_values_northing\n",
    "\n",
    "# easting\n",
    "y_true = UTM_predictions['easting']\n",
    "y_pred = UTM_predictions['easting_pred_smooth']\n",
    "error = mean_absolute_error(y_true, y_pred)\n",
    "print(f'Easting smoothed MAE = {error}')\n",
    "\n",
    "y_true = UTM_predictions['easting']\n",
    "y_pred = UTM_predictions['easting_pred']\n",
    "error = mean_absolute_error(y_true, y_pred)\n",
    "print(f'Easting unsmoothed MAE = {error}')\n",
    "\n",
    "# northing\n",
    "y_true = UTM_predictions['northing']\n",
    "y_pred = UTM_predictions['northing_pred_smooth']\n",
    "error = mean_absolute_error(y_true, y_pred)\n",
    "print(f'Northing smoothed MAE = {error}')\n",
    "\n",
    "y_true = UTM_predictions['northing']\n",
    "y_pred = UTM_predictions['northing_pred']\n",
    "error = mean_absolute_error(y_true, y_pred)\n",
    "print(f'Northing unsmoothed MAE = {error}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### these plots would be better at the point of the tower estimates\n",
    "\n",
    "# Plot the estimated vs actual predictions\n",
    "plt.scatter(y_true, y_pred)\n",
    "plt.xlabel(\"Actual y_test Values\")\n",
    "plt.ylabel(\"Predicted y_test Values\")\n",
    "plt.title(\"Actual vs Predicted Values for y_test\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the residuals\n",
    "residuals = y_true - y_pred\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_true, residuals)\n",
    "plt.xlabel(\"Actual y_test Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Break to generate exports for checking data\n",
    "\n",
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_xy(x, y):\n",
    "    plt.plot(x, y, 'o')\n",
    "    plt.xlabel('xOffset')\n",
    "    plt.ylabel('yOffset')\n",
    "    plt.title('XY plot of xOffset vs. yOffset')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot to check for outliers\n",
    "df = train_data\n",
    "\n",
    "sim_data_preproc = preprocess_sim_data(df, data_type, freq, tower_locs)\n",
    "x = sim_data_preproc['xOffset']\n",
    "y = sim_data_preproc['yOffset']\n",
    "plot_xy(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = 'H:\\My Drive\\Colab Notebooks\\RadioTelemetry\\Simul_data\\Processed_RTdat_w_LatLong_append\\\\test_predictions_check_wSD_wNA_20230402.xlsx'\n",
    "\n",
    "test_predictions.to_excel(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c9c44150708f88dcc61b6a40c517040b165081f9891458623fe805b4ae9321d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
